{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Base Model Evaluation\n",
    "\n",
    "\n",
    "## Overview\n",
    "Evaluation is a crucial step allowing us to measure the performance and generalization ability of our models. In this notebook, we will systematically evaluate the base model using appropriate metrics and validation datasets before fine tuning it on BMO data.\n",
    "\n",
    "- **Base Model Evaluation:**  \n",
    "    We will assess the initial performance of the base model to establish a benchmark. This helps us understand how well the model performs before any task-specific adaptation.\n",
    "\n",
    "\n",
    "Throughout this notebook, we will use visualizations and quantitative metrics to provide a comprehensive analysis of model performance. This approach ensures transparency and helps guide further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Setup Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\"\n",
    "\n",
    "MODEL_NAME = os.getenv(\"STUDENT_MODEL_NAME\", \"RedHatAI/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Create the output directory if not available\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = OUTPUT_DIR / \"base_model\" / MODEL_NAME.replace(\"/\", \"__\")\n",
    "\n",
    "\n",
    "print(f\"Model name : {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Save the model locally, for easier access for following steps\n",
    "if not MODEL_PATH.exists():\n",
    "    print(\"Model not available locally, Downloading the model locally \")\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Loading model {MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}\")\n",
    "\n",
    "    # Save the tokenizer\n",
    "    print(f\"Loading tokenizer {MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "    print(f\"Tokenizer saved to {MODEL_PATH}\")\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "else:\n",
    "    print(f\"Model Available locally : {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, dtype=torch.float16, device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"âœ… Successfully Loaded the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## LLM Sampling Parameters\n",
    "\n",
    "We will use the below when testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# ðŸŽ¯ Sampling/Generation Parameters                                            #\n",
    "################################################################################\n",
    "MAX_NEW_TOKENS = 256\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.7  # Meta's recommended temperature for Llama\n",
    "TOP_P = 0.9  # Standard top_p for Llama models\n",
    "\n",
    "print(f\"MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\n",
    "print(f\"DO_SAMPLE: {DO_SAMPLE}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"TOP_P: {TOP_P}\")\n",
    "print(\"âœ… LLM sampling parameters defined\")\n",
    "print()\n",
    "print(\"ðŸ“Š Using Meta's recommended Llama sampling settings:\")\n",
    "print(\"  â€¢ Temperature 0.6 for balanced creativity/consistency\")\n",
    "print(\"  â€¢ Top-p 0.9 for good token diversity\")\n",
    "print(\"  â€¢ Stop on both EOS and <|eot_id|> tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pprint\n",
    "\n",
    "\n",
    "def prompt_runner(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_experimentation(prompt):\n",
    "    # Run prompt in base model\n",
    "    base_model_response = prompt_runner(model=model, tokenizer=tokenizer, prompt=prompt)\n",
    "\n",
    "    # Print the response of the model\n",
    "\n",
    "    pprint(f\"\"\"\n",
    "    [bold]EXPERIMENTATION DETAILS[/bold]:\n",
    "        MODEL NAME     : {MODEL_NAME}\n",
    "        MAX NEW TOKENS : {MAX_NEW_TOKENS}\n",
    "        DO SAMPLE      : {DO_SAMPLE}\n",
    "        TEMPERATURE    : {TEMPERATURE}\n",
    "        TOP P          : {TOP_P}\n",
    "\n",
    "\n",
    "    [bold]PROMPT ðŸ’¬[/bold]:\n",
    "\n",
    "        [green]{prompt}[/green]\n",
    "\n",
    "    [bold]MODEL RESPONSE ðŸ¤–[/bold]:\n",
    "\n",
    "        {base_model_response}\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "\n",
    "We will test the knowledge of the model on BMO data. \n",
    "\n",
    "Question: `what is the meaning of verifying the identity of a person or an entity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is the meaning of verifying the identity of a person or an entity\"\"\"\n",
    "\n",
    "\n",
    "run_experimentation(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03-knowledge-mixing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
