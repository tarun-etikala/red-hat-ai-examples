{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dfca11",
   "metadata": {},
   "source": [
    "# Base model evaluation\n",
    "\n",
    "\n",
    "## Overview\n",
    "Evaluation is a crucial step in the Knowldedge Tuning workflow. It allows you to measure the performance and generalization ability of your model. In this notebook, you systematically evaluate the base model by using appropriate metrics and validation datasets before you fine tune the model on the example Bank of Montreal (BMO) data.\n",
    "\n",
    "Throughout this notebook, you use visualizations and quantitative metrics to analyze performance of the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93a0ae",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364dc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c323ab",
   "metadata": {},
   "source": [
    "## Set up paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6129be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\"\n",
    "\n",
    "MODEL_NAME = os.getenv(\"STUDENT_MODEL_NAME\", \"RedHatAI/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Create the output directory if does not exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = OUTPUT_DIR / \"base_model\" / MODEL_NAME.replace(\"/\", \"__\")\n",
    "\n",
    "\n",
    "print(f\"Model name : {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc2db5",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fe897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Save the model locally, for easier access in the following steps\n",
    "if not MODEL_PATH.exists():\n",
    "    print(\"Model not available locally, Downloading the model locally \")\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Loading model {MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    print(f\"Model saved to {MODEL_PATH}\")\n",
    "\n",
    "    # Save the tokenizer\n",
    "    print(f\"Loading tokenizer {MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "    print(f\"Tokenizer saved to {MODEL_PATH}\")\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "else:\n",
    "    print(f\"Model Available locally : {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ad5b2",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2dcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, dtype=torch.float16, device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"âœ… Successfully loaded the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1d66b",
   "metadata": {},
   "source": [
    "## LLM sampling parameters\n",
    "\n",
    "Use the following parameters to test the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# ðŸŽ¯ Sampling/Generation Parameters                                            #\n",
    "################################################################################\n",
    "MAX_NEW_TOKENS = 256\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.7  # Meta's recommended temperature for Llama\n",
    "TOP_P = 0.9  # Standard top_p for Llama models\n",
    "\n",
    "print(f\"MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\n",
    "print(f\"DO_SAMPLE: {DO_SAMPLE}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"TOP_P: {TOP_P}\")\n",
    "print(\"âœ… LLM sampling parameters defined\")\n",
    "print()\n",
    "print(\"ðŸ“Š Using Meta's recommended Llama sampling settings:\")\n",
    "print(\"  â€¢ Temperature 0.6 for balanced creativity/consistency\")\n",
    "print(\"  â€¢ Top-p 0.9 for good token diversity\")\n",
    "print(\"  â€¢ Stop on both EOS and <|eot_id|> tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e4629",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pprint\n",
    "\n",
    "\n",
    "def prompt_runner(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_experimentation(prompt):\n",
    "    # Run prompt in base model\n",
    "    base_model_response = prompt_runner(model=model, tokenizer=tokenizer, prompt=prompt)\n",
    "\n",
    "    # Print the response of the model\n",
    "\n",
    "    pprint(f\"\"\"\n",
    "    [bold]EXPERIMENTATION DETAILS[/bold]:\n",
    "        MODEL NAME     : {MODEL_NAME}\n",
    "        MAX NEW TOKENS : {MAX_NEW_TOKENS}\n",
    "        DO SAMPLE      : {DO_SAMPLE}\n",
    "        TEMPERATURE    : {TEMPERATURE}\n",
    "        TOP P          : {TOP_P}\n",
    "\n",
    "\n",
    "    [bold]PROMPT ðŸ’¬[/bold]:\n",
    "\n",
    "        [green]{prompt}[/green]\n",
    "\n",
    "    [bold]MODEL RESPONSE ðŸ¤–[/bold]:\n",
    "\n",
    "        {base_model_response}\n",
    "\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d221b",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "\n",
    "Test the knowledge of the base model on BMO data:\n",
    "\n",
    "Question: `what is the meaning of verifying the identity of a person or an entity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is the meaning of verifying the identity of a person or an entity\"\"\"\n",
    "\n",
    "\n",
    "run_experimentation(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03-knowledge-mixing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
