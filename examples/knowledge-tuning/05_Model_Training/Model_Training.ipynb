{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to fine-tune large language models (LLMs) using orthogonal subspace fine-tuning (OSFT) to add new capabilities without losing existing knowledge.\n",
    "\n",
    "Fine-tuning language models is hard‚Äîyou need good data, lots of resources, and even small changes can cause problems. This makes it tough to add new abilities to a model. This problem is called **continual learning** and is what our new training technique, orthogonal subspace fine-tuning (OSFT), solves.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "- A GPU-enabled environment with sufficient resources for model training.\n",
    "- Combined training datasets from the [Knowledge Mixing step](../04_Knowledge_Mixing/Knowledge_mixing.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Ensure the following after running the below code cell:\n",
    "- Ensure you have `training_hub` installed with cuda support.\n",
    "- Ensure you have the right `flash_attn` version for your CUDA setup.\n",
    "\n",
    "üö® Ensure you dont have `vllm` installed, as this conflicts with `training_hub` package required for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "if importlib.util.find_spec(\"vllm\") is not None:\n",
    "    print(\n",
    "        \"‚ö†Ô∏è vllm installation found! Please uninstall vllm as it may cause dependecies issues with other packages required for this step\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ No vllm installation found! Continuing with installing dependencies ...\")\n",
    "    !pip install -qqU .\n",
    "    from utils.flash_attn_installer import download_flash_attention_wheel\n",
    "\n",
    "    flash_attn_wheel_path = download_flash_attention_wheel()\n",
    "    if flash_attn_wheel_path:\n",
    "        print(f\"\\nInstalling flash-attn from {flash_attn_wheel_path} ...\")\n",
    "        !pip install -qqU {flash_attn_wheel_path}\n",
    "        print(\"‚úÖ Flash-attn wheel installed successfully!\")\n",
    "    else:\n",
    "        print(\n",
    "            \"‚ùå Flash-attn wheel download failed. Please check the previous logs for errors.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Setup Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_05\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create output directory if it doesn't exist\n",
    "\n",
    "KNOWLEDGE_MIXED_DATASET_PATH = WORKSPACE / \"output\" / \"step_04\" / \"training_mix\"\n",
    "\n",
    "################################################################################\n",
    "# ü§ñ Model + Data Paths                                                        #\n",
    "################################################################################\n",
    "BASE_MODEL_NAME = os.getenv(\"STUDENT_MODEL_NAME\", \"RedHatAI/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Path to the Training Dataset from Step 04\n",
    "# Select the appropriate dataset with the cut size to use for training\n",
    "DATASET_PATH = KNOWLEDGE_MIXED_DATASET_PATH / \"combined_cut_5x.jsonl\"\n",
    "CHECKPOINTS_PATH = OUTPUT_DIR / \"checkpoint\"\n",
    "DATA_OUTPUT_PATH = (\n",
    "    OUTPUT_DIR / \"dev\" / \"shm\"\n",
    ")  # for quicker multi-process loading of datasets\n",
    "\n",
    "\n",
    "BASE_MODEL_PATH = (\n",
    "    WORKSPACE / \"output\" / \"base_model\" / BASE_MODEL_NAME.replace(\"/\", \"__\")\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Base Model Name : {BASE_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL LOCALLY\n",
    "\n",
    "if not BASE_MODEL_PATH.exists():\n",
    "    print(\"Model not available locally, Downloading the model locally \")\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Loading model {BASE_MODEL_NAME}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "    model.save_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"Model saved to {BASE_MODEL_PATH}\")\n",
    "\n",
    "    # Save the tokenizer\n",
    "    print(f\"Loading tokenizer {BASE_MODEL_NAME}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    tokenizer.save_pretrained(BASE_MODEL_PATH)\n",
    "    print(f\"Tokenizer saved to {BASE_MODEL_PATH}\")\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "else:\n",
    "    print(f\"Model Available locally : {BASE_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import and configure everything that we need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training_hub for OSFT training\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "from io import StringIO\n",
    "\n",
    "import torch\n",
    "from training_hub import osft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Logging Configuration\n",
    "\n",
    "Set up logging to track progress while preventing notebook crashes from excessive output.\n",
    "<!-- \n",
    "**Note:** For production workflows or long-running jobs, we recommend using the script version at `scripts/osft-training.py` for better logging consistency and resumption capabilities.\n",
    "\n",
    "**Quick script usage:**\n",
    "```bash\n",
    "python scripts/lab_multiphase_osft_training.py \\\n",
    "  --base-model-path /path/to/model \\\n",
    "  --phase07-data-path /path/to/knowledge.jsonl \\\n",
    "  --phase10-data-path /path/to/skills.jsonl \\\n",
    "  --ckpt-output-base-dir /path/to/checkpoints -->\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to show only essential information\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "# Suppress verbose logging from transformers and other libraries\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"‚úÖ Logging configured for notebook environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Let's define some helper functions for checkpoint management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "def find_most_recent_checkpoint(output_dir):\n",
    "    \"\"\"\n",
    "    Find the most recent checkpoint in the training output directory.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): Training output directory containing hf_format/ subdirectory\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the most recent checkpoint\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no checkpoints are found\n",
    "    \"\"\"\n",
    "    # Get all checkpoint directories under hf_format\n",
    "    checkpoint_pattern = os.path.join(output_dir, \"hf_format\", \"samples_*.0\")\n",
    "    checkpoint_dirs = glob.glob(checkpoint_pattern)\n",
    "\n",
    "    if not checkpoint_dirs:\n",
    "        raise ValueError(\n",
    "            f\"No checkpoints found in {os.path.join(output_dir, 'hf_format')}\"\n",
    "        )\n",
    "\n",
    "    # Find the most recently created checkpoint\n",
    "    most_recent_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "\n",
    "    return most_recent_checkpoint\n",
    "\n",
    "\n",
    "def save_best_model(source_directory):\n",
    "    \"\"\"This function copies the final model from the checkpoints dir to the base output dir for easier access.\n",
    "\n",
    "    Args:\n",
    "        source_directory: Path to the recent checkpoint\n",
    "    \"\"\"\n",
    "    FINAL_FINE_TUNED_MODEL_PATH = (\n",
    "        WORKSPACE / \"output\" / \"fine_tuned_model\" / BASE_MODEL_NAME.replace(\"/\", \"__\")\n",
    "    )\n",
    "    FINAL_FINE_TUNED_MODEL_PATH.mkdir(\n",
    "        exist_ok=True, parents=True\n",
    "    )  # Create the directory if not available\n",
    "\n",
    "    # Iterate through all files/folders in source and copy them\n",
    "    for item in os.listdir(source_directory):\n",
    "        src_path = os.path.join(source_directory, item)\n",
    "        dst_path = os.path.join(FINAL_FINE_TUNED_MODEL_PATH, item)\n",
    "\n",
    "        # Copy directories or files appropriately\n",
    "        if os.path.isdir(src_path):\n",
    "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"‚úÖ Final finetuned model copied\\n\\t Path :{FINAL_FINE_TUNED_MODEL_PATH}\")\n",
    "    return FINAL_FINE_TUNED_MODEL_PATH\n",
    "\n",
    "\n",
    "def cleanup_model_memory(*objects):\n",
    "    \"\"\"\n",
    "    Clean up GPU memory by deleting arbitrary objects and clearing CUDA cache.\n",
    "\n",
    "    Args:\n",
    "        *objects: Variable number of objects to clean up (models, tokenizers, etc.)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Delete all provided objects\n",
    "    # Delete objects from global namespace if they exist there\n",
    "    for obj in objects:\n",
    "        if obj is not None:\n",
    "            # Find the variable name in globals that references this object\n",
    "            for var_name, var_obj in list(globals().items()):\n",
    "                if var_obj is obj:\n",
    "                    print(f\"üóëÔ∏è Deleting global variable: {var_name}\")\n",
    "                    del globals()[var_name]\n",
    "                    break\n",
    "            # Also delete the local reference\n",
    "            del obj\n",
    "\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Clear CUDA cache if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    print(\"‚úÖ Model memory cleaned up and CUDA cache cleared\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Checkpoint utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "For this example, I will be running training on an 8xA100 box, but these hyperparameters can be adjusted for any machine; provided it is capable of running OSFT.\n",
    "\n",
    "\n",
    "üö® Ensure you have configured the number of GPUs available on the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Best Practices: Choosing Your Unfreeze Rank Ratio\n",
    "\n",
    "The `unfreeze_rank_ratio` is your key control for balancing learning vs. preservation. Here's how to choose the right value for your use case.\n",
    "\n",
    "### Choosing the Right Unfreeze Rank Ratio\n",
    "============================================================\n",
    "\n",
    "1Ô∏è‚É£  **Small Behavior Tweaks**\n",
    "   When to use: Adjusting specific model behaviors without major changes\n",
    "   \n",
    "   Examples: Output formatting, response style, minor corrections\n",
    "\n",
    "   üéØ Strategy: Start SMALL\n",
    "   - unfreeze_rank_ratio = 0.1 - 0.15\n",
    "   - Why: Minimal modification preserves most model behavior\n",
    "   - Result: Targeted changes without broad impact\n",
    "\n",
    "2Ô∏è‚É£  **Major New Capabilities**\n",
    "   When to use: Adding entirely new skills or knowledge domains\n",
    "   \n",
    "   Examples: New language, coding ability, domain expertise\n",
    "\n",
    "   üéØ Strategy: Start STANDARD\n",
    "   - unfreeze_rank_ratio = 0.3 - 0.35\n",
    "   - Why: More freedom to learn complex new patterns\n",
    "   - Result: Robust new capabilities while preserving base model\n",
    "\n",
    "3Ô∏è‚É£  **Sequential Task Learning (Task 1 ‚Üí Task 2)**\n",
    "   When to use: Training on multiple tasks in sequence\n",
    "   \n",
    "   Examples: Knowledge ‚Üí Skills, General ‚Üí Specialized\n",
    "\n",
    "   üéØ Strategy: PROGRESSIVELY REDUCE\n",
    "   - Task 1: unfreeze_rank_ratio = 0.35\n",
    "   - Task 2: unfreeze_rank_ratio = 0.30 (reduce by 0.05)\n",
    "   - Task 3: unfreeze_rank_ratio = 0.25 (reduce by 0.05)\n",
    "   - Why: Each reduction preserves previous learning\n",
    "   - Result: Accumulate capabilities without forgetting\n",
    "\n",
    "üåü Golden Rules:\n",
    "\n",
    "- Never go below 0.1 (too restrictive for learning)\n",
    "- Never go above 0.5 (risks forgetting)\n",
    "- When in doubt, start smaller - you can always increase\n",
    "- Test preservation after each training phase\n",
    "\n",
    "üìä **Quick Reference:**\n",
    "\n",
    "\n",
    "| Use Case               | Recommended Ratio | Notes                    |\n",
    "|------------------------|-------------------|--------------------------|\n",
    "| Format tweaks          | 0.10 - 0.15       | Minimal changes          |\n",
    "| Style adjustments      | 0.15 - 0.20       | Moderate refinement      |\n",
    "| New domain knowledge   | 0.25 - 0.35       | Major capability         |\n",
    "| New task type          | 0.30 - 0.35       | Significant learning     |\n",
    "| Sequential phase 2+    | Previous - 0.05   | Preserve prior phases    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Training Hyperparameters                                                  #\n",
    "################################################################################\n",
    "# Important for OSFT\n",
    "UNFREEZE_RANK_RATIO = 0.25\n",
    "\n",
    "# Standard parameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 5e-6\n",
    "NUM_EPOCHS = 2\n",
    "LR_SCHEDULER = \"cosine\"\n",
    "WARMUP_STEPS = 0\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# üèéÔ∏è Performance Hyperparameters                                               #\n",
    "################################################################################\n",
    "USE_LIGER = True\n",
    "MAX_TOKENS_PER_GPU = 10_000\n",
    "MAX_SEQ_LEN = 8192\n",
    "\n",
    "################################################################################\n",
    "# üíæ Checkpointing Settings                                                    #\n",
    "################################################################################\n",
    "# Here we only want to save the very last checkpoint\n",
    "SAVE_FINAL_CHECKPOINT = True\n",
    "CHECKPOINT_AT_EPOCH = False\n",
    "\n",
    "################################################################################\n",
    "# üî• TORCHRUN SETTINGS                                                         #\n",
    "################################################################################\n",
    "NUM_GPUS = 8  # Configure the number of GPUs that are allocated for training\n",
    "NUM_NODES = 1\n",
    "NODE_RANK = 0\n",
    "RDZV_ID = 23\n",
    "RDZV_ENDPOINT = \"localhost:1738\"\n",
    "\n",
    "if torch.cuda.device_count() < NUM_GPUS:\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è  Warning: Available GPUs ({torch.cuda.device_count()}) less than requested ({NUM_GPUS}). Adjusting NUM_GPUS.\"\n",
    "    )\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    print(f\"Using NUM_GPUS={NUM_GPUS}\\n\")\n",
    "print(\"‚öôÔ∏è  Training Hyperparameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base Model: {BASE_MODEL_NAME}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"Checkpoints Path: {CHECKPOINTS_PATH}\")\n",
    "print(f\"Data Output Path: {DATA_OUTPUT_PATH}\")\n",
    "print()\n",
    "print(f\"Unfreeze Rank Ratio: {UNFREEZE_RANK_RATIO}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"LR Scheduler: {LR_SCHEDULER}\")\n",
    "print(f\"Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"Seed: {SEED}\")\n",
    "print()\n",
    "print(f\"Use Liger: {USE_LIGER}\")\n",
    "print(f\"Max Tokens per GPU: {MAX_TOKENS_PER_GPU:,}\")\n",
    "print(f\"Max Sequence Length: {MAX_SEQ_LEN:,}\")\n",
    "print()\n",
    "print(f\"Save Final Checkpoint: {SAVE_FINAL_CHECKPOINT}\")\n",
    "print(f\"Checkpoint at Epoch: {CHECKPOINT_AT_EPOCH}\")\n",
    "print()\n",
    "print(\n",
    "    f\"Distributed: {NUM_GPUS} GPUs √ó {NUM_NODES} nodes = {NUM_GPUS * NUM_NODES} total GPUs\"\n",
    ")\n",
    "print(f\"Node Rank: {NODE_RANK}\")\n",
    "print(f\"RDZV ID: {RDZV_ID}\")\n",
    "print(f\"RDZV Endpoint: {RDZV_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Fine-tuning the model using OSFT\n",
    "\n",
    "Since prompting the model doesn't work, our next step is modifying the model.\n",
    "Normally, this wouldn't be a great solution since many methods can cause the model to forget important capabilities. \n",
    "However; OSFT allows us to adjust the non-critical pieces of the model while keeping the crucial parts intact -- perfect for our use-case üòÉ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Training with OSFT\n",
    "\n",
    "With our hyperparameters configured, now we launch a training job and sit back while it enhances our new model üòéüçø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting OSFT Continual Learning Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting from: {BASE_MODEL_NAME}\")\n",
    "print(f\"Training data: {DATASET_PATH}\")\n",
    "print(f\"Output directory: {CHECKPOINTS_PATH}\")\n",
    "print(f\"Unfreeze ratio: {UNFREEZE_RANK_RATIO}\")\n",
    "print()\n",
    "\n",
    "# Capture output to prevent notebook crashes\n",
    "output_buffer = StringIO()\n",
    "error_buffer = StringIO()\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with redirect_stdout(output_buffer), redirect_stderr(error_buffer):\n",
    "        # OSFT training\n",
    "        training_result = osft(\n",
    "            # Model and data\n",
    "            model_path=str(BASE_MODEL_PATH),\n",
    "            data_path=str(DATASET_PATH),\n",
    "            ckpt_output_dir=str(CHECKPOINTS_PATH),\n",
    "            # OSFT-specific\n",
    "            unfreeze_rank_ratio=UNFREEZE_RANK_RATIO,\n",
    "            # Training parameters\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            effective_batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            max_seq_len=MAX_SEQ_LEN,\n",
    "            max_tokens_per_gpu=MAX_TOKENS_PER_GPU,\n",
    "            # Data processing\n",
    "            data_output_dir=str(DATA_OUTPUT_PATH),\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            # Optimization\n",
    "            use_liger=USE_LIGER,\n",
    "            seed=SEED,\n",
    "            lr_scheduler=LR_SCHEDULER,\n",
    "            # Checkpointing\n",
    "            checkpoint_at_epoch=CHECKPOINT_AT_EPOCH,\n",
    "            save_final_checkpoint=SAVE_FINAL_CHECKPOINT,\n",
    "            # Distributed training\n",
    "            nproc_per_node=NUM_GPUS,\n",
    "            nnodes=NUM_NODES,\n",
    "            node_rank=NODE_RANK,\n",
    "            rdzv_id=RDZV_ID,\n",
    "            rdzv_endpoint=RDZV_ENDPOINT,\n",
    "        )\n",
    "\n",
    "    training_duration = time.time() - training_start_time\n",
    "\n",
    "    # Find the most recent checkpoint from Phase10 training\n",
    "    final_checkpoint = find_most_recent_checkpoint(CHECKPOINTS_PATH)\n",
    "    print(\"\\n\\n\\n‚úÖ Model Training Completed\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÅ Final model checkpoint: {final_checkpoint}\")\n",
    "\n",
    "    fine_tuned_model_path = save_best_model(final_checkpoint)\n",
    "\n",
    "    print(f\"‚úÖ OSFT training completed  in {training_duration / 3600:.2f} hours!\")\n",
    "    print()\n",
    "    print(\"üìä Training Achievements:\")\n",
    "    print(\"  ‚Ä¢ Base model capabilities: ‚úÖ Preserved\")\n",
    "    print(\"  ‚Ä¢ New knowledge integrated: ‚úÖ Complete\")\n",
    "    print(\"  ‚Ä¢ Continual learning: ‚úÖ Success\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå OSFT training failed: {e}\")\n",
    "    print(\"\\nError details:\")\n",
    "    print(error_buffer.getvalue())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Model training has been completed successfully!\n",
    "You can now proceed to the [Evaluation](../06_Evaluation/Evaluation.ipynb) notebook to assess the performance of your fine-tuned model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
