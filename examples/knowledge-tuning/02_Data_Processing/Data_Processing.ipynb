{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f458de",
   "metadata": {},
   "source": [
    "# Create seed data\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook takes chunks of a source document and combines the chunks with In Context Learning (ICL) fields to create a `seed_data.jsonl` file for the [knowledge generation notebook](../03_Knowledge_Generation/Knowledge_Generation.ipynb).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- The URL of the source document.\n",
    "- A snippet of the source document that is approximately 500 tokens in size. This snippet is used as the `icl_document` in the following code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace22335",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6af06",
   "metadata": {},
   "source": [
    "## Setup paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f755561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_02\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create output directory if it does not exist\n",
    "\n",
    "\n",
    "DOCLING_OUTPUT_DIR = OUTPUT_DIR / \"docling_output\"\n",
    "DOCLING_OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create docling output directory if it does not exist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74f433",
   "metadata": {},
   "source": [
    "## Generate Docling document\n",
    "\n",
    "Convert the source document into Markdown format by using Docling. In this example, the source document is a webpage for the Bank of Montreal (BMO) website.\n",
    "\n",
    "For documentation on supported file types and usage, see [the Docling documentation](https://docling-project.github.io/docling/usage/supported_formats/).\n",
    "\n",
    "\n",
    "#### Source doument:\n",
    "- [BMO webpage](https://fintrac-canafe.canada.ca/guidance-directives/client-clientele/Guide11/11-eng)\n",
    "- ðŸš¨ [Terms and Conditions](https://www.canada.ca/en/transparency/terms.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df629a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "WEB_URLS = [\n",
    "    (\n",
    "        \"BMO_data\",\n",
    "        \"https://fintrac-canafe.canada.ca/guidance-directives/client-clientele/Guide11/11-eng\",\n",
    "    )\n",
    "]\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "for name, url in WEB_URLS:\n",
    "    result = converter.convert(url)\n",
    "    result.document.save_as_markdown(f\"{DOCLING_OUTPUT_DIR}/{name}.md\")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Number of md files in {DOCLING_OUTPUT_DIR}: \",\n",
    "    len(glob.glob(f\"{DOCLING_OUTPUT_DIR}/*.md\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9892143-eddf-4612-aa70-eead88140f86",
   "metadata": {},
   "source": [
    "## Load the converted document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c0562-ace2-44e7-9884-de95de03ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(glob.glob(f\"{DOCLING_OUTPUT_DIR}/*.md\")[0]) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f0068f",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from markdown_it import MarkdownIt\n",
    "\n",
    "\n",
    "def chunk_markdown(text: str, max_tokens: int = 200, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits Markdown text into chunks at block-level elements\n",
    "    (headings, paragraphs, lists, tables, code, blockquotes).\n",
    "    Adds overlap (in words) between all consecutive chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The markdown text to be chunked\n",
    "        max_tokens: Maximum number of words per chunk\n",
    "        overlap: Number of overlapping words between consecutive chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks with specified overlap\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Markdown parser to understand the document structure\n",
    "    md = MarkdownIt()\n",
    "    tokens = md.parse(text)\n",
    "\n",
    "    # To ensure that you do not split the text in the middle of headings or lists,\n",
    "    # group tokens into block-level segments to preserve the Markdown structure\n",
    "    blocks = []\n",
    "    buf = []\n",
    "    for tok in tokens:\n",
    "        if tok.block and tok.type.endswith(\"_open\"):\n",
    "            buf = []\n",
    "        elif tok.block and tok.type.endswith(\"_close\"):\n",
    "            if buf:\n",
    "                blocks.append(\"\\n\".join(buf).strip())\n",
    "                buf = []\n",
    "        elif tok.content:\n",
    "            buf.append(tok.content)\n",
    "    if buf:\n",
    "        blocks.append(\"\\n\".join(buf).strip())\n",
    "\n",
    "    # Split blocks into chunks with overlap to maintain context continuity\n",
    "    chunks = []\n",
    "    current_words = []\n",
    "    for block in blocks:\n",
    "        words = block.split()\n",
    "        for w in words:\n",
    "            current_words.append(w)\n",
    "            if len(current_words) >= max_tokens:\n",
    "                # Emit a complete chunk\n",
    "                chunks.append(\" \".join(current_words))\n",
    "                # Prepare next buffer with overlap from the end of this chunk\n",
    "                # to ensure context continuity between chunks\n",
    "                current_words = current_words[-overlap:] if overlap > 0 else []\n",
    "\n",
    "    # Add any remaining words as the final chunk\n",
    "    if current_words:\n",
    "        chunks.append(\" \".join(current_words))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_chunks_to_jsonl(chunks, filename):\n",
    "    \"\"\"\n",
    "    Save a list of strings to a JSONL file where each line is a JSON object\n",
    "    with the key 'chunk'. Returns the path to the saved file.\n",
    "\n",
    "    Args:\n",
    "        chunks (list of str): List of text chunks to save.\n",
    "        filename (str): Path to the output .jsonl file (string or Path).\n",
    "\n",
    "    Returns:\n",
    "        pathlib.Path: Path to the saved file.\n",
    "    \"\"\"\n",
    "    path = Path(filename)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk in chunks:\n",
    "            json_line = json.dumps({\"chunk\": chunk}, ensure_ascii=False)\n",
    "            f.write(json_line + \"\\n\")\n",
    "    print(f\"Saved {len(chunks)} chunks to {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67177b-d35c-419d-880f-d066d91f2afc",
   "metadata": {},
   "source": [
    "## Chunk Markdown\n",
    "\n",
    "The `chunk_markdown` utility function chunks the Markdown file into smaller pieces.\n",
    "\n",
    "Run the following command to break down the Markdown files into chunks that are at least `max_tokens` in length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97789ea-434d-41cd-832b-aad957ab1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_markdown(text, max_tokens=5000, overlap=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2d9e7-0ccb-45d9-9cd8-bc2457a5c9ab",
   "metadata": {},
   "source": [
    "## (Optional) Save chunks to an intermediate chunks.jsonl file\n",
    "\n",
    "You can save chunks to an intermediate `chunks.jsonl` file. You can use this intermediate file to tweak the chunks before you create the seed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b856acc-aecf-4a83-83af-5dccc326bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_path = save_chunks_to_jsonl(chunks, f\"{OUTPUT_DIR}/chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c6af0-d094-448d-85dc-4599a0937374",
   "metadata": {},
   "source": [
    "## (Optional) Review size of chunks\n",
    "\n",
    "For the purpose of this example, chunks should be between 6-8K tokens in length. Run the following code to check whether chunks (excluding the final chunk) are within this range. If they are not within this range, merge or split chunks until they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153287d-19ed-42a9-9d1f-bf793fdfdad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "i = 1\n",
    "min_tokens = 6000\n",
    "max_tokens = 8000\n",
    "for chunk in chunks:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = len(enc.encode(chunk))\n",
    "    if (token_count < min_tokens or token_count > max_tokens) and (i != len(chunks)):\n",
    "        print(\n",
    "            f\"\\033[31mWARNING: Chunk {i} ({chunk[:30]} ... {chunk[-30:]}) {token_count} tokens\\033[0m\"\n",
    "        )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45260e-b816-430f-98c1-c0d98adb3f30",
   "metadata": {},
   "source": [
    "## Load chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc7c2b-6957-44c1-811b-2e950a540e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "chunks_files = [f\"{OUTPUT_DIR}/chunks.jsonl\"]\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "chunks = (\n",
    "    load_dataset(\"json\", data_files=chunks_files)\n",
    "    .rename_columns({\"chunk\": \"document\"})\n",
    "    .select_columns(\"document\")\n",
    ")\n",
    "# chunks is a DatasetDict. By default, the dataset for the chunks is put in the \"train\" split in the DatasetDict\n",
    "chunks = chunks[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a445a5-1497-4058-8431-c81e7ee9e06d",
   "metadata": {},
   "source": [
    "## Set ICL fields\n",
    "\n",
    "The seed data requires the following fields:\n",
    "   - `document_outline`: A concise title or summary that accurately represents the entire document. For documents that cover multiple themes, provide an outline for section.\n",
    "   - `domain`: The domain or subject area of the document.\n",
    "   - `icl_document`: A ~500 token representative sample extracted from the document. The sample can include paragraphs, bulleted lists, tables, code snippets, and definitions.\n",
    "   - `icl_query_1`, `icl_query_2`, `icl_query_3`: Three questions that are based on the `icl_document` sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_outline = \"IFINTRAC's compliance guidance\"\n",
    "\n",
    "domain = \"Finance\"\n",
    "\n",
    "icl_document = \"\"\"You can use a Canadian credit file as one of the two pieces of information required to verify the identity of a person under the dual-process method. Specifically, you can use it to confirm the person's name and address, name and date of birth, or to confirm the person's name and confirm that the person has a credit card account or a loan account. If you use a credit file as one of the information pieces for the dual-process method, it must have existed for at least six months.\n",
    "\n",
    "You must use a second source, for example, a property tax assessment, to confirm the second category of information. In this instance, the two reliable sources are the Canadian credit bureau that provided the credit file information and the municipal government that issued the property tax assessment. The information from these two sources must match the information provided by the person.\n",
    "\n",
    "You can also refer to information from a Canadian credit bureau if it acts as an aggregator that compiles information from different reliable sources (often referred to as tradelines). In this instance, the Canadian credit bureau must provide information from **two** independent tradelines, where each tradeline confirms one of the two categories of information required to verify the identity of a person under this method. In this instance, **each tradeline is a distinct source; the credit bureau is not the source**.\n",
    "\"\"\"\n",
    "\n",
    "icl_query_1 = \"What specific information from a Canadian credit file can I use to verify a person's identity under the dual-process method?\"\n",
    "icl_query_2 = \"What are the requirements for the second source of information when using a credit file as one of the two pieces for identity verification?\"\n",
    "icl_query_3 = \"When a Canadian credit bureau acts as an aggregator of information from multiple tradelines, what conditions must be met for the information to satisfy the dual-process method requirements?\"\n",
    "\n",
    "\n",
    "icl = {\n",
    "    \"document_outline\": document_outline,\n",
    "    \"icl_document\": icl_document,\n",
    "    \"icl_query_1\": icl_query_1,\n",
    "    \"icl_query_2\": icl_query_2,\n",
    "    \"icl_query_3\": icl_query_3,\n",
    "    \"domain\": domain,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335ab8b-13b4-4279-8f20-b2830f11d5c2",
   "metadata": {},
   "source": [
    "## Map ICL fields to document chunks and write the `seed_data.jsonl` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2f6fb-bc92-48d2-9794-1f14ca6804ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the ICL fields to each document chunk (if you want to use the same ICL for all, as shown here)\n",
    "seed_data = chunks.map(lambda x: icl)\n",
    "\n",
    "# Save the seed data to a JSONL file \n",
    "seed_data.to_json(f\"{OUTPUT_DIR}/seed_data.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b775f0c",
   "metadata": {},
   "source": [
    "The `seed_data.jsonl` file is now ready for you to use in the next step of the knowledge tuning example workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3ff7f",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "- Open the [Knowledge Generation](../03_Knowledge_Generation/Knowledge_Generation.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03-knowledge-mixing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
