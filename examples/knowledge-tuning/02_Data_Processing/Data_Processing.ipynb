{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Seed Data Creation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook takes chunks of a source document and combines them with In Context Learning (ICL) fields to create a seed_data.jsonl file for the [knowledge generation notebook](../03_Knowledge_Generation/Knowledge_Generation.ipynb).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Markdown (.md) file(s) of the source document.\n",
    "- A snippet of the source document that is around 500 tokens in size. This will get used as the `icl_document` below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Setup Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_02\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create output directory if it doesn't exist\n",
    "\n",
    "\n",
    "DOCLING_OUTPUT_DIR = OUTPUT_DIR / \"docling_output\"\n",
    "DOCLING_OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create docling output directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Generate Docling Document\n",
    "\n",
    "Convert the source document (PDF, DOCX,HTML, etc.) into markdown format using Docling\n",
    "\n",
    "\n",
    "This example works through the conversion of BMO Website from a URL to markdown using Docling.\n",
    "You can find more documentation on supported file types and usage [here](https://docling.readthedocs.io/en/latest/).\n",
    "\n",
    "\n",
    "#### Data Description\n",
    "- Source Document: [BMO Webpage](https://fintrac-canafe.canada.ca/guidance-directives/client-clientele/Guide11/11-eng)\n",
    "    - ðŸš¨ [Terms and Conditions](https://www.canada.ca/en/transparency/terms.html)\n",
    "\n",
    "\n",
    "NOTE: If you already have the docling markdown file(s) of the source document, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "WEB_URLS = [\n",
    "    (\n",
    "        \"BMO_data\",\n",
    "        \"https://fintrac-canafe.canada.ca/guidance-directives/client-clientele/Guide11/11-eng\",\n",
    "    )\n",
    "]\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "for name, url in WEB_URLS:\n",
    "    result = converter.convert(url)\n",
    "    result.document.save_as_markdown(f\"{DOCLING_OUTPUT_DIR}/{name}.md\")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Number of md files in {DOCLING_OUTPUT_DIR}: \",\n",
    "    len(glob.glob(f\"{DOCLING_OUTPUT_DIR}/*.md\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Load Converted Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're coming with a docling JSON instead of markdown the following lines will help you convert docling JSON -> .md\n",
    "# converter = DocumentConverter()\n",
    "# result = converter.convert(\"document_collection/ibm-annual-report/ibm-annual-report-2024.json\")\n",
    "# result.document.save_as_markdown(\"document_collection/ibm-annual-report/ibm-annual-report-2024.md\")\n",
    "# print(\"Markown saved to document_collection/ibm-annual-report/ibm-annual-report-2024.md\")\n",
    "\n",
    "\n",
    "# In our example above docling step produces markdown of all the pdf files in the document_collection\n",
    "with open(glob.glob(f\"{DOCLING_OUTPUT_DIR}/*.md\")[0]) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from markdown_it import MarkdownIt\n",
    "\n",
    "\n",
    "def chunk_markdown(text: str, max_tokens: int = 200, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits Markdown text into chunks at block-level elements\n",
    "    (headings, paragraphs, lists, tables, code, blockquotes).\n",
    "    Adds overlap (in words) between all consecutive chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The markdown text to be chunked\n",
    "        max_tokens: Maximum number of words per chunk\n",
    "        overlap: Number of overlapping words between consecutive chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks with specified overlap\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize markdown parser to understand document structure\n",
    "    md = MarkdownIt()\n",
    "    tokens = md.parse(text)\n",
    "\n",
    "    # Group tokens into block-level segments to preserve markdown structure\n",
    "    # This ensures we don't split in the middle of headings, lists, etc.\n",
    "    blocks = []\n",
    "    buf = []\n",
    "    for tok in tokens:\n",
    "        if tok.block and tok.type.endswith(\"_open\"):\n",
    "            buf = []\n",
    "        elif tok.block and tok.type.endswith(\"_close\"):\n",
    "            if buf:\n",
    "                blocks.append(\"\\n\".join(buf).strip())\n",
    "                buf = []\n",
    "        elif tok.content:\n",
    "            buf.append(tok.content)\n",
    "    if buf:\n",
    "        blocks.append(\"\\n\".join(buf).strip())\n",
    "\n",
    "    # Split blocks into chunks with overlap to maintain context continuity\n",
    "    chunks = []\n",
    "    current_words = []\n",
    "    for block in blocks:\n",
    "        words = block.split()\n",
    "        for w in words:\n",
    "            current_words.append(w)\n",
    "            if len(current_words) >= max_tokens:\n",
    "                # Emit a complete chunk\n",
    "                chunks.append(\" \".join(current_words))\n",
    "                # Prepare next buffer with overlap from the end of this chunk\n",
    "                # This ensures context continuity between chunks\n",
    "                current_words = current_words[-overlap:] if overlap > 0 else []\n",
    "\n",
    "    # Add any remaining words as the final chunk\n",
    "    if current_words:\n",
    "        chunks.append(\" \".join(current_words))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_chunks_to_jsonl(chunks, filename):\n",
    "    \"\"\"\n",
    "    Save a list of strings to a JSONL file where each line is a JSON object\n",
    "    with the key 'chunk'. Returns the Path to the saved file.\n",
    "\n",
    "    Args:\n",
    "        chunks (list of str): List of text chunks to save.\n",
    "        filename (str): Path to the output .jsonl file (string or Path).\n",
    "\n",
    "    Returns:\n",
    "        pathlib.Path: Path to the saved file.\n",
    "    \"\"\"\n",
    "    path = Path(filename)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk in chunks:\n",
    "            json_line = json.dumps({\"chunk\": chunk}, ensure_ascii=False)\n",
    "            f.write(json_line + \"\\n\")\n",
    "    print(f\"Saved {len(chunks)} chunks to {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Chunk Markdown\n",
    "\n",
    "Markdown files will be broken down into chunks at least `max_tokens` in length.\n",
    "\n",
    "Utilize the utility function `chunk_markdown` to chunk the markdown file into smaller pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_markdown(text, max_tokens=5000, overlap=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## (Optional) Save Chunks to intermediate chunks.jsonl\n",
    "\n",
    "The intermediate `chunks.jsonl` file can be used to tweak chunks before proceeding to seed dataset creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_path = save_chunks_to_jsonl(chunks, f\"{OUTPUT_DIR}/chunks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## (Optional) Review size of Chunks\n",
    "\n",
    "Chunks should be between 6-8K tokens in length. Chunks that are not within this range (excluding the final chunk) should be merged or split apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "i = 1\n",
    "min_tokens = 6000\n",
    "max_tokens = 8000\n",
    "for chunk in chunks:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = len(enc.encode(chunk))\n",
    "    if (token_count < min_tokens or token_count > max_tokens) and (i != len(chunks)):\n",
    "        print(\n",
    "            f\"\\033[31mWARNING: Chunk {i} ({chunk[:30]} ... {chunk[-30:]}) {token_count} tokens\\033[0m\"\n",
    "        )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Load Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "chunks_files = [f\"{OUTPUT_DIR}/chunks.jsonl\"]\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "chunks = (\n",
    "    load_dataset(\"json\", data_files=chunks_files)\n",
    "    .rename_columns({\"chunk\": \"document\"})\n",
    "    .select_columns(\"document\")\n",
    ")\n",
    "# chunks is a DatasetDict. By default the Dataset for the chunks is getting put in the \"train\" split in the DatasetDict\n",
    "chunks = chunks[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Set ICL Fields\n",
    "\n",
    "The seed data requires the following fields:\n",
    "   - `document_outline`: A concise title or summary that accurately represents the entire document.\n",
    "     For documents covering multiple themes, consider providing multiple outlines (one per section).\n",
    "   - `domain`: The domain or subject area of the document.\n",
    "   - `icl_document`: A ~500 token representative sample extracted from the document. This may include paragraphs, bulleted lists, tables, code snippets, definitions, etc.\n",
    "   - `icl_query_1`, `icl_query_2`, `icl_query_3`: Three questions based on the `icl_document` sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_outline = \"IFINTRAC's compliance guidance\"\n",
    "\n",
    "domain = \"Finance\"\n",
    "\n",
    "icl_document = \"\"\"A Canadian credit file can be used as one of the two pieces of information  required to verify the identity of a person under the dual-process method. Specifically,  it can be used to confirm the person's name and address, name and date of birth,  or to confirm the person's name and confirm that the person has a credit card account  or a loan account. If you use a credit file as one of the information pieces for the dual-process  method, it must have existed for at least six months.\n",
    "\n",
    "Information from a second source, for example, a property tax assessment,  must be used to confirm the second category of information. In this instance,  the two reliable sources are the Canadian credit bureau that provided the  credit file information and the municipal government that issued the property  tax assessment. The information from these two sources must match the  information provided by the person.\n",
    "\n",
    "You can also refer to information from a Canadian credit bureau if it  acts as an aggregator that compiles information from different reliable sources  (often referred to as tradelines). In this instance, the Canadian credit bureau must  provide you with information from **two** independent tradelines where each tradeline confirms one of the two categories  of information required to verify the identity of a person under this method. In  this instance, **each tradeline is a distinct  source; the credit bureau is not the source** .\n",
    "\"\"\"\n",
    "\n",
    "icl_query_1 = \"What specific information can a Canadian credit file be used to confirm when verifying a person's identity under the dual-process method?\"\n",
    "icl_query_2 = \"What are the requirements for the second source of information when using a credit file as one of the two pieces for identity verification?\"\n",
    "icl_query_3 = \"When a Canadian credit bureau acts as an aggregator of information from multiple tradelines, what conditions must be met for the information to satisfy the dual-process method requirements?\"\n",
    "\n",
    "\n",
    "icl = {\n",
    "    \"document_outline\": document_outline,\n",
    "    \"icl_document\": icl_document,\n",
    "    \"icl_query_1\": icl_query_1,\n",
    "    \"icl_query_2\": icl_query_2,\n",
    "    \"icl_query_3\": icl_query_3,\n",
    "    \"domain\": domain,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Map ICL Fields to Document Chunks and Write `seed_data.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the ICL fields to each document chunk (if you want to use the same ICL for all, as shown here)\n",
    "seed_data = chunks.map(lambda x: icl)\n",
    "\n",
    "# Save the seed data to a JSONL file for downstream use\n",
    "seed_data.to_json(f\"{OUTPUT_DIR}/seed_data.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "- The seed_data.jsonl file is now ready for the knowledge tuning pipeline.\n",
    "- You can now refer to the [knowledge generation](../03_Knowledge_Generation/Knowledge_Generation.ipynb) notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03-knowledge-mixing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
