{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate knowledge tuning data\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how you can generate high-quality knowledge tuning datasets using the Red Hat AI Synthetic Data Generation (SDG) Hub framework. It creates multiple types of document augmentations and corresponding question and answer (Q&A) pairs that you can use to train or fine-tune language models for enhanced summarization and knowledge extraction capabilities.\n",
    "\n",
    "This notebook generates the following types of knowledge tuning datasets:\n",
    "- **Extractive summaries**: Concise summaries that extract key information directly from source documents.\n",
    "- **Detailed summaries**: Coprehensive summaries that provide thorough coverage of document content.\n",
    "- **Key facts**: Structured fact extraction with corresponding Q&A pairs.\n",
    "- **Document-based Q&A**: Question-answer pairs generated directly from document content.\n",
    "\n",
    "This notebook outputs structured training data. For each augmentation, it saves a JSONL dataset. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- You completed the [Data preprocessing ](../03_Knowledge_Generation/Knowledge_Generation.ipynb) notebook.\n",
    "\n",
    "- The following environment variables are set in the [.env.example](./.env.example) file:\n",
    "  - `TEACHER_MODEL_API_KEY` â€” The LLM API key.\n",
    "  - `TEACHER_MODEL_BASE_URL` â€” The LLM HTTP endpoint.\n",
    "  - `TEACHER_MODEL_NAME` â€” The model to call for data generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "âš ï¸ NOTE: If you use vllm to serve your teacher model, edit the following Python command to include the vllm option:\n",
    "\n",
    "`pip install -qqU .[vllm]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\" / \"step_03\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(\n",
    "    parents=True, exist_ok=True\n",
    ")  # Create the output directory if it doesn't exist\n",
    "\n",
    "# Create the output directory\n",
    "dataset_output_folders = [\n",
    "    \"extractive_summary\",\n",
    "    \"detailed_summary\",\n",
    "    \"key_facts_to_qa\",\n",
    "    \"document_based_qa\",\n",
    "]\n",
    "for dataset_folder in dataset_output_folders:\n",
    "    dataset_output_dir = OUTPUT_DIR / dataset_folder\n",
    "    dataset_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ… Created output directory for {dataset_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the teacher model\n",
    "\n",
    "You can use your own inference server for the teacher model. You can also host your own teacher model by using `vllm` in a seperate terminal and then use the available GPUs.\n",
    "\n",
    "ðŸš¨ The following example command runs the inference server by using vllm. Refer to the https://docs.redhat.com/en/documentation/red_hat_ai_inference_server(vllm documentation) for information about configuring parameters.\n",
    "\n",
    "\n",
    "```bash\n",
    "vllm serve RedHatAI/gpt-oss-120b \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --served-model-name teacher \\\n",
    "    --tensor-parallel-size=8 \\\n",
    "    --distributed-executor-backend=mp \\\n",
    "    --max-model-len=128000 \\\n",
    "```\n",
    "\n",
    "To view the supported teacher models for the SDG Hub flow, use the `flow.get_model_recommendations()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = os.getenv(\"TEACHER_MODEL_NAME\", \"openai/gpt-oss-120b\")\n",
    "API_KEY = os.getenv(\"TEACHER_MODEL_API_KEY\", \"None\")  # Provide your API key here\n",
    "ENDPOINT = os.getenv(\"TEACHER_MODEL_BASE_URL\", \"http://0.0.0.0:8000/v1\")\n",
    "\n",
    "print(f\"MODEL NAME : {MODEL_NAME}\")\n",
    "print(f\"API_KEY : {API_KEY}\")\n",
    "print(f\"ENDPOINT : {ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to run the flow with async mode\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "RUN_ON_VALIDATION_SET = True  # Whether to run the model on the validation set\n",
    "SEED_DATA_SUBSAMPLE = 0  # Subsample the seed data for faster testing.\n",
    "\n",
    "SEED_DATA_FILE = (\n",
    "    WORKSPACE / \"output\" / \"step_02\" / \"seed_data.jsonl\"\n",
    ")  # Path to the seed data file generated in step 2\n",
    "\n",
    "\n",
    "ENABLE_REASONING = False  # Whether to enable reasoning in the model\n",
    "NUMBER_OF_SUMMARIES = 50  # Number of summaries to generate\n",
    "MAX_CONCURRENCY = 50  # Maximum number of concurrent requests to the model\n",
    "\n",
    "# Set the timeout for the teacher model inference server used by sdg flows. Default: 120\n",
    "INFERENCE_SERVER_TIMEOUT = 120\n",
    "\n",
    "\n",
    "os.environ[\"LITELLM_REQUEST_TIMEOUT\"] = str(INFERENCE_SERVER_TIMEOUT)\n",
    "if not SEED_DATA_FILE.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"\\nNot a valid seed data ! {SEED_DATA_FILE}.\\nPlease run step 2 to generate the seed data. \\n(or) Provide the correct path to the seed data file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Party\n",
    "\n",
    "# First Party\n",
    "from sdg_hub import Flow, FlowRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seed data generated when you ran the Data Processing notebook\n",
    "quality_corpus = pd.read_json(SEED_DATA_FILE, lines=True)\n",
    "\n",
    "if SEED_DATA_SUBSAMPLE > 0:\n",
    "    quality_corpus = quality_corpus.iloc[:SEED_DATA_SUBSAMPLE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SDG\n",
    "\n",
    "Run the following code to create a knowledge flow from the provided YAML file. For the purposes of this example, the dataset is small.\n",
    "\n",
    "For large scale generation, use the python command provided in the next cell.\n",
    "You can analyze the generated data to check that the quality is similar to the provided Q&A pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover the available generation flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-discover all available flows (no set up needed!)\n",
    "FlowRegistry.discover_flows()\n",
    "\n",
    "# List the available flows\n",
    "flows = FlowRegistry.list_flows()\n",
    "print(f\"Available flows: {flows}\")\n",
    "\n",
    "# Search the flows by tag\n",
    "qa_flows = FlowRegistry.search_flows(tag=\"question-generation\")\n",
    "print(f\"QA flows: {qa_flows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the extractive summaries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for extractive summary\n",
    "flow_name = \"Extractive Summary Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "# Generate data for extractive summary\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        \"question_generation\": {\"max_tokens\": 1024},\n",
    "        \"gen_extractive_summary\": {\"n\": NUMBER_OF_SUMMARIES, \"max_tokens\": 6000},\n",
    "    }\n",
    "else:\n",
    "    runtime_params = {\"gen_extractive_summary\": {\"n\": NUMBER_OF_SUMMARIES}}\n",
    "\n",
    "extractive_summary_generated_data = flow.generate(\n",
    "    quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY\n",
    ")\n",
    "\n",
    "extractive_summary_generated_data.to_json(\n",
    "    os.path.join(OUTPUT_DIR, \"extractive_summary\", \"gen.jsonl\"),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Extractive summary: {len(extractive_summary_generated_data)} records\")\n",
    "\n",
    "print(f\"âœ“ Columns: {list(extractive_summary_generated_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the detailed summary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate similar data for Detailed Summary\n",
    "flow_name = \"Detailed Summary Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        \"question_generation\": {\"max_tokens\": 1024},\n",
    "        \"gen_detailed_summary\": {\"n\": NUMBER_OF_SUMMARIES, \"max_tokens\": 6000},\n",
    "    }\n",
    "else:\n",
    "    runtime_params = {\"gen_detailed_summary\": {\"n\": NUMBER_OF_SUMMARIES}}\n",
    "# Generate data for detailed summary\n",
    "detailed_summary_generated_data = flow.generate(\n",
    "    quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY\n",
    ")\n",
    "\n",
    "detailed_summary_generated_data.to_json(\n",
    "    os.path.join(OUTPUT_DIR, \"detailed_summary\", \"gen.jsonl\"),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Detailed summary: {len(detailed_summary_generated_data)} records\")\n",
    "\n",
    "print(f\"âœ“ Columns: {list(detailed_summary_generated_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the key facts dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate similar data for key facts\n",
    "flow_name = \"Key Facts Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "runtime_params = {}\n",
    "\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens for Question Generation to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        \"generate_key_fact_qa\": {\"max_tokens\": 6000},\n",
    "    }\n",
    "\n",
    "# Generate data for key facts summary\n",
    "key_facts_generated_data = flow.generate(\n",
    "    quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY\n",
    ")\n",
    "\n",
    "key_facts_generated_data.to_json(\n",
    "    os.path.join(OUTPUT_DIR, \"key_facts_to_qa\", \"gen.jsonl\"),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Key facts: {len(key_facts_generated_data)} records\")\n",
    "\n",
    "print(f\"âœ“ Columns: {list(key_facts_generated_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the document-based dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_name = \"Document Based Knowledge Tuning Dataset Generation Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "flow = Flow.from_yaml(flow_path)\n",
    "\n",
    "# Set model configuration\n",
    "flow.set_model_config(\n",
    "    model=MODEL_NAME,\n",
    "    api_base=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "runtime_params = {}\n",
    "\n",
    "if ENABLE_REASONING:\n",
    "    # Increase max tokens to accommodate reasoning content\n",
    "    runtime_params = {\n",
    "        \"question_generation\": {\"max_tokens\": 2048},\n",
    "    }\n",
    "\n",
    "document_based_generated_data = flow.generate(\n",
    "    quality_corpus, runtime_params=runtime_params, max_concurrency=MAX_CONCURRENCY\n",
    ")\n",
    "\n",
    "document_based_generated_data.to_json(\n",
    "    os.path.join(OUTPUT_DIR, \"document_based_qa\", \"gen.jsonl\"),\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Document based: {len(document_based_generated_data)} records\")\n",
    "\n",
    "print(f\"âœ“ Columns: {list(document_based_generated_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ‰ You now have all four of the document augmentations (detailed summaries, extractive summaries, key facts and document-based) along with their corresponding Q&A pairs.\n",
    "\n",
    "## Next step\n",
    "  \n",
    "   In the [Knowledge mixing notebook](../04_Knowledge_Mixing/Knowledge_Mixing.ipynb), you combine and curate these datasets to prepare your final training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-knowledge-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
