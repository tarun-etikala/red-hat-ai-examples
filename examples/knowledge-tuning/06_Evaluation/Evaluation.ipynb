{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dfca11",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "## Overview\n",
    "Evaluation is a crucial step in model training. It allows you to measure the performance and generalization ability of our models. In this notebook, you use appropriate metrics and validation datasets to systematically evaluate both the base model and the fine-tuned model.\n",
    "\n",
    "- **Base Model Evaluation:**  \n",
    "    Assess the initial performance of the base model to establish a benchmark. This benchmark helps you understand how the model performs before any task-specific adaptation.\n",
    "\n",
    "- **Finetuned Model Evaluation:**  \n",
    "    After training the model on your specific dataset, evaluate its performance again. By comparing these results with the base model results, you can see the improvements that fine-tuning provides.\n",
    "\n",
    "To ensure transparency and to help guide further improvements, this notebook, uses visualizations and quantitative metrics to provide a comprehensive analysis of model performance. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- You completed the [Model Training](../05_Model_Training/Model_Training.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93a0ae",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364dc634",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c323ab",
   "metadata": {},
   "source": [
    "## Set up paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6129be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WORKSPACE = Path.cwd().parent  # Path to the workspace directory\n",
    "\n",
    "\n",
    "MODEL_NAME = os.getenv(\"STUDENT_MODEL_NAME\", \"RedHatAI/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "OUTPUT_DIR = WORKSPACE / \"output\"\n",
    "\n",
    "BASE_MODEL_PATH = OUTPUT_DIR / \"base_model\" / MODEL_NAME.replace(\"/\", \"__\")\n",
    "\n",
    "FINE_TUNED_MODEL_PATH = OUTPUT_DIR / \"fine_tuned_model\" / MODEL_NAME.replace(\"/\", \"__\")\n",
    "\n",
    "if not BASE_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(\"ðŸš¨ Base model directory doesn't exist.\")\n",
    "\n",
    "if not FINE_TUNED_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(\"ðŸš¨ Finetuned model directory doesn't exist.\")\n",
    "\n",
    "\n",
    "print(f\"Model Name : {MODEL_NAME}\")\n",
    "print(f\"Base model path : {BASE_MODEL_PATH}\")\n",
    "print(f\"Finetuned model path : {FINE_TUNED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc2db5",
   "metadata": {},
   "source": [
    "## Load the base model and the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fe897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading base model and tokenizer...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH, dtype=torch.float16, device_map=\"cuda:0\"\n",
    ")\n",
    "base_model_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "print(\"âœ… Base model and tokenizer loaded successfully.\")\n",
    "\n",
    "\n",
    "print(\"Loading fine-tuned model and tokenizer...\")\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    FINE_TUNED_MODEL_PATH, dtype=torch.float16, device_map=\"cuda:0\"\n",
    ")\n",
    "fine_tuned_model_tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH)\n",
    "print(\"âœ… Fine-tuned model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1d66b",
   "metadata": {},
   "source": [
    "## LLM sampling parameters\n",
    "\n",
    "Define parameter values for LLM sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# ðŸŽ¯ Sampling/Generation Parameters                                            #\n",
    "################################################################################\n",
    "MAX_NEW_TOKENS = 256\n",
    "DO_SAMPLE = True\n",
    "TEMPERATURE = 0.7  # Meta's recommended temperature for Llama\n",
    "TOP_P = 0.9  # Standard top_p for Llama models\n",
    "\n",
    "print(f\"MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\n",
    "print(f\"DO_SAMPLE: {DO_SAMPLE}\")\n",
    "print(f\"TEMPERATURE: {TEMPERATURE}\")\n",
    "print(f\"TOP_P: {TOP_P}\")\n",
    "print(\"âœ… LLM sampling parameters defined\")\n",
    "print()\n",
    "print(\"ðŸ“Š Using Meta's recommended Llama sampling settings:\")\n",
    "print(\"  â€¢ Temperature 0.6 for balanced creativity/consistency\")\n",
    "print(\"  â€¢ Top-p 0.9 for good token diversity\")\n",
    "print(\"  â€¢ Stop on both EOS and <|eot_id|> tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e4629",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as pprint\n",
    "\n",
    "\n",
    "def prompt_runner(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            do_sample=DO_SAMPLE,\n",
    "            top_p=TOP_P,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def run_experimentation(prompt):\n",
    "    # Run prompt in base model\n",
    "    base_model_response = prompt_runner(\n",
    "        model=base_model, tokenizer=base_model_tokenizer, prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Run prompt in fine_tuned_model\n",
    "    fine_tuned_model_response = prompt_runner(\n",
    "        model=fine_tuned_model, tokenizer=fine_tuned_model_tokenizer, prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Print the response of the model\n",
    "\n",
    "    pprint(f\"\"\"\n",
    "    [bold]EXPERIMENTATION DETAILS[/bold]:\n",
    "        MODEL NAME     : {MODEL_NAME}\n",
    "        MAX NEW TOKENS : {MAX_NEW_TOKENS}\n",
    "        DO SAMPLE      : {DO_SAMPLE}\n",
    "        TEMPERATURE    : {TEMPERATURE}\n",
    "        TOP P          : {TOP_P}\n",
    "\n",
    "\n",
    "    [bold]PROMPT ðŸ’¬[/bold]:\n",
    "\n",
    "        [green]{prompt}[/green]\n",
    "\n",
    "    [bold]BASE MODEL RESPONSE ðŸ¤–[/bold]:\n",
    "\n",
    "        {base_model_response}\n",
    "\n",
    "    [bold]FINE TUNED MODEL RESPONSE ðŸ¤–[/bold]:\n",
    "\n",
    "        {fine_tuned_model_response}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d221b",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Use the following question to test the model's knowledge of Bank of Montreal (BMO) data:\n",
    "\n",
    "Question: `what is the meaning of verifying the identity of a person or an entity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"what is the meaning of verifying the identity of a person or an entity\"\"\"\n",
    "\n",
    "\n",
    "run_experimentation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb240a58",
   "metadata": {},
   "source": [
    "Congratulations! You have completed the Knowledge Tuning example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
