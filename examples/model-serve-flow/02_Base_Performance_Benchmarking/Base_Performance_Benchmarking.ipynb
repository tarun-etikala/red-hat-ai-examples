{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4d68b1-dd85-4056-ac9a-752d04b49b54",
   "metadata": {},
   "source": [
    "## Base model performance benchmarking with GuideLLM\n",
    "\n",
    "This notebook evaluates the system-level performance of the **base model**. It uses GuideLLM, an open-source benchmarking tool designed to measure the performance of large language models deployed with **vLLM**. The results from this notebook serve as a baseline for comparing performance against the compressed model deployment. \n",
    "\n",
    "**Goal**\n",
    "\n",
    "Establish baseline performance metrics for the base model to compare with the performance of the compressed model under identical serving conditions.\n",
    "\n",
    "**Key actions**:\n",
    "\n",
    "- Start a vLLM server to host the compressed model.\n",
    "\n",
    "- Collect metrics such as token throughput, Time to First Token (TTFT), Inter-Token Latency (ITL), and End-to-end request latency.\n",
    "\n",
    "- Save results in JSON format.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Performance metrics for the base model.\n",
    "\n",
    "- A baseline to use for comparison against the compressed model's performance.\n",
    "\n",
    "For details on system level performance benchmarking and GuideLLM, see [Performance Benchmarking with GuideLLM](../docs/System_Level_Performance_Benchmarking.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2c741",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8d94d-10ad-4ed4-aba0-a3c71ca4254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from guidellm.benchmark import GenerativeBenchmarksReport\n",
    "from utils import generate, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9effa5",
   "metadata": {},
   "source": [
    "### Check GPU memory\n",
    "\n",
    "To make sure that you have enough GPU memory to run this notebook:\n",
    "\n",
    "1. In a terminal window, run the `nvidia-smi` command.\n",
    "\n",
    "2. If there are processes that are using GPU memory that this notebook requires, run the `kill -9 <pid>` command for each process to stop it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7574b-94f4-4ad2-b035-5d2399cba89c",
   "metadata": {},
   "source": [
    "### Launch an inference server (vLLM) for the base model\n",
    "\n",
    "Set up a vLLM inference server to host your base model and to expose an OpenAI-compatible API endpoint. This server is required so that GuideLLM can benchmark system-level performance, such as throughput, latency, and time-to-first-token. In Module 6, you can compare the performance benchmarks of the base and compressed models.\n",
    "\n",
    "**Resources used** : 46GB L40S GPU x 1\n",
    "\n",
    "For more information about using vLLM, see [Launch Inference Servers for the Base and Compressed Models using vLLM](../docs/Model_Serving_vLLM.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccaa0bf",
   "metadata": {},
   "source": [
    "####  Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level for vLLM inference\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4a108",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Run the vLLM server\n",
    "\n",
    "This example uses the configuration for a single-node, single-GPU set up to launch a vLLM server for the base model. \n",
    "\n",
    "1. Open a terminal window.\n",
    "   \n",
    "2. Make sure that the working directory is the `02_Base_Performance_Benchmarking` directory.\n",
    "   \n",
    "   `pwd`\n",
    "   \n",
    "3. Run the following command to serve the base model by using vLLM:\n",
    "\n",
    "```bash\n",
    "vllm serve \\\n",
    "  \"../base_model/RedHatAI-Llama-3.1-8B-Instruct\" \\\n",
    "  --host 127.0.0.1 \\\n",
    "  --port 8000 \\\n",
    "  --gpu-memory-utilization 0.6 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --pipeline-parallel-size 1 \\\n",
    "  --max-model-len 2048\n",
    "```\n",
    "\n",
    "4. Wait for the server to start, as indicated by the following status updates:\n",
    "\n",
    "    ```INFO:     Started server process [166518]```\\\n",
    "    ```INFO:     Waiting for application startup.```\\\n",
    "    ```INFO:     Application startup complete.```\n",
    "\n",
    "**NOTE:** You can safely ignore the following warning message:\n",
    "\n",
    "`The tokenizer you are loading from '../base_model/RedHatAI-Llama-3.1-8B-Instruct' with an incorrect regex pattern... This will lead to incorrect tokenization.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1d4e9",
   "metadata": {},
   "source": [
    "#### Verify that the vLLM server is accessible\n",
    "\n",
    "Use the  **generate** helper function (defined in [utils.py](./utils.py)) to simplify sending requests to the locally-served vLLM model.\n",
    "\n",
    "This function wraps the OpenAI-compatible Chat Completions API exposed by vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd308e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"../base_model/RedHatAI-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867ca69-cb68-4f24-a099-14563ddd4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non streaming results\n",
    "response = generate(\n",
    "    model=base_model_path,\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8000,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae275bdd-8f0f-4ccb-8337-d2cadd69cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming results\n",
    "res = \"\"\n",
    "for chunk in stream(\n",
    "    model=base_model_path,\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8000,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    "):\n",
    "    res += chunk\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0f061-08cd-4bbc-95e2-1dbb3456b543",
   "metadata": {},
   "source": [
    "### Run performance benchmarking\n",
    "\n",
    "After you verify that the vLLM server for the base model is running, you can proceed with benchmarking the base model's performance by using **GuideLLM**.\n",
    "\n",
    "1. Identify values for the following parameters:\n",
    "\n",
    "   - **target**: The URL of the vLLM inference server started in the previous step, for example: `http://127.0.0.1:8000` If the vLLM server is running on a different port, update the `target` accordingly.\n",
    "\n",
    "   - **output-path**: The path to save the benchmarking results.\n",
    "\n",
    "2. Open a terminal window.\n",
    "   \n",
    "4. Make sure that the working directory is the `02_Base_Performance_Benchmarking` directory.\n",
    "\n",
    "   `pwd`\n",
    "   \n",
    "6. Check the following command and, if needed, edit the **target** and **output-path** parameters. Then run the command in the terminal.\n",
    "\n",
    "    ```bash\n",
    "    guidellm benchmark \\\n",
    "      --target \"http://127.0.0.1:8000\" \\\n",
    "      --profile sweep \\\n",
    "      --max-seconds 120 \\\n",
    "      --data \"prompt_tokens=1024,output_tokens=512\" \\\n",
    "      --output-path \"../results/base_performance_benchmarks.json\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f7854-3d9d-4124-8be2-31a2e4fad8ed",
   "metadata": {},
   "source": [
    "#### View the results\n",
    "\n",
    "The result of the `guidellm benchmark` command is formatted in multiple tables. The results display in the terminal and are saved to the directory defined by `output-path`.\n",
    "\n",
    "1. **Request Latency Statistics (Completed Requests)**\n",
    "\n",
    "This table focuses on how **long** requests take and the latency characteristics of the server.\n",
    "\n",
    "```text\n",
    "\n",
    "ℹ Request Latency Statistics (Completed Requests)\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "| Benchmark   | Request Latency || TTFT             || ITL        || TPOT         ||\n",
    "| Strategy    | Sec             || ms               || ms         || ms           ||\n",
    "|             | Mdn     | p95    | Mdn     | p95     | Mdn  | p95  | Mdn   | p95   |\n",
    "|-------------|---------|--------|---------|---------|------|------|-------|-------|\n",
    "| synchronous | 11.4    | 11.4   | 115.9   | 124.1   | 22.2 | 22.2 | 22.3  | 22.4  |\n",
    "| throughput  | 62.3    | 92.4   | 33854.1 | 60812.2 | 55.7 | 92.4 | 121.6 | 180.5 |\n",
    "| constant    | 12.5    | 12.6   | 130.4   | 143.1   | 24.3 | 24.3 | 24.5  | 24.5  |\n",
    "| constant    | 13.2    | 13.2   | 133.6   | 144.9   | 25.6 | 25.6 | 25.8  | 25.8  |\n",
    "| constant    | 14.0    | 14.1   | 133.5   | 144.9   | 27.1 | 27.2 | 27.3  | 27.5  |\n",
    "| constant    | 14.7    | 14.8   | 138.7   | 151.9   | 28.6 | 28.6 | 28.8  | 28.9  |\n",
    "| constant    | 16.5    | 16.6   | 140.2   | 156.9   | 32.0 | 32.2 | 32.3  | 32.4  |\n",
    "| constant    | 17.5    | 17.5   | 143.2   | 157.9   | 33.9 | 34.0 | 34.1  | 34.3  |\n",
    "| constant    | 18.5    | 18.7   | 146.8   | 161.8   | 36.0 | 36.3 | 36.2  | 36.5  |\n",
    "| constant    | 20.4    | 20.4   | 147.0   | 162.4   | 39.6 | 39.7 | 39.8  | 39.9  |\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "```\n",
    "\n",
    "2.  **Server Throughput Statistics**\n",
    "\n",
    "This table focuses on how many requests a server can handle per second. Throughput can be thought of as the **rate** (or time required) of processing. \n",
    "```text\n",
    "ℹ Server Throughput Statistics\n",
    "|=============|=====|======|=======|=======|========|========|========|=======|=======|========|\n",
    "| Benchmark   | Requests                |||| Input Tokens   || Output Tokens || Total Tokens  ||\n",
    "| Strategy    | Per Sec   || Concurrency  || Per Sec        || Per Sec       || Per Sec       ||\n",
    "|             | Mdn | Mean | Mdn   | Mean  | Mdn    | Mean   | Mdn    | Mean  | Mdn   | Mean   |\n",
    "|-------------|-----|------|-------|-------|--------|--------|--------|-------|-------|--------|\n",
    "| synchronous | 0.1 | 0.1  | 1.0   | 1.0   | 92.5   | 101.8  | 45.1   | 44.8  | 45.1  | 137.4  |\n",
    "| throughput  | 0.5 | 1.7  | 125.0 | 100.5 | 132.2  | 3094.4 | 604.5  | 885.0 | 607.5 | 2713.7 |\n",
    "| constant    | 0.3 | 0.2  | 3.0   | 3.2   | 303.9  | 314.3  | 138.2  | 135.9 | 138.3 | 416.8  |\n",
    "| constant    | 0.5 | 0.4  | 6.0   | 5.7   | 519.6  | 530.3  | 204.8  | 227.9 | 205.2 | 698.9  |\n",
    "| constant    | 0.7 | 0.6  | 10.0  | 8.6   | 735.7  | 746.1  | 262.5  | 318.9 | 262.7 | 977.8  |\n",
    "| constant    | 0.9 | 0.8  | 13.0  | 11.5  | 951.1  | 962.4  | 344.7  | 408.1 | 344.8 | 1251.4 |\n",
    "| constant    | 1.1 | 0.9  | 18.0  | 15.5  | 1169.7 | 1178.3 | 422.8  | 491.6 | 423.4 | 1507.6 |\n",
    "| constant    | 1.3 | 1.1  | 23.0  | 19.3  | 1383.0 | 1394.3 | 464.6  | 576.5 | 465.1 | 1767.8 |\n",
    "| constant    | 1.5 | 1.3  | 28.0  | 23.4  | 1598.5 | 1610.6 | 497.3  | 658.7 | 498.2 | 2019.9 |\n",
    "| constant    | 1.7 | 1.4  | 34.0  | 28.5  | 1827.6 | 1826.5 | 576.5  | 734.0 | 577.6 | 2250.6 |\n",
    "|=============|=====|======|=======|=======|========|========|========|=======|=======|========|\n",
    "\n",
    "```\n",
    "#### Base Model Performance Summary\n",
    "1. Max concurrency under load: 34.0 (Concurrency Mdn)\n",
    "2. Max output tokens per second under load: 576.5 (Output tokens per sec Mdn)\n",
    "3. Request latency under load: 20.4 (Request Latency in secs Mdn)\n",
    "4. Time to first token under load: 147.0 (TTFT ms Mdn)\n",
    "5. Inter token latency under load: 39.6 (ITL ms Mdn)\n",
    "\n",
    "#### SLO Analysis\n",
    "\n",
    "Assume the Service Level Objective (SLO) is:\n",
    "\n",
    "    TTFT ≤ 200 milliseconds for 95% of requests (p95) with optimal concurrency\n",
    "\n",
    "Given the SLO of TTFT ≤ 200 ms for 95% of requests (p95) at optimal concurrency, the base model meets this requirement. At the highest tested concurrency of 34 requests, the p95 TTFT is 162.4 ms, which satisfies the SLO.\n",
    "\n",
    "These results establish the performance baseline for the base model. After you compress the model, you can benchmark it under the same conditions to determine whether model compression leads to improved TTFT while continuing to meet the SLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0c53b-3f15-460a-a6ea-785e3eb92609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after the benchmarking process in the terminal completes\n",
    "report = GenerativeBenchmarksReport.load_file(\n",
    "    path=\"../results/base_performance_benchmarks.json\",\n",
    ")\n",
    "base_benchmarks = report.benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8ca06-42c8-4487-b0ac-de3a65449ad4",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "base_benchmarks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
