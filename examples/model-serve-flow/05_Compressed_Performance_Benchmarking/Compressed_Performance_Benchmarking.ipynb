{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Compressed model performance benchmarking with GuideLLM\n",
    "\n",
    "This notebook evaluates the system-level performance of the **compressed model**. It uses GuideLLM, an open-source benchmarking tool designed to measure the performance of large language models deployed with **vLLM**. You can use the results of this notebook to understand how compression affects latency, throughput, and scalability compared to the uncompressed baseline. \n",
    "\n",
    "**Goal**\n",
    "\n",
    "Assess the performance efficiency of the compressed model and quantify the gains or trade-offs introduced by compression under realistic serving conditions.\n",
    "\n",
    "**Key actions**:\n",
    "\n",
    "- Start a vLLM server to host the compressed model.\n",
    "  \n",
    "- Check GPU vRAM.\n",
    "\n",
    "- Collect metrics such as token throughput, Time to First Token (TTFT), Inter-Token Latency (ITL), and End-to-end request latency.\n",
    "\n",
    "- Save results in JSON format.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Performance metrics for the compressed model.\n",
    "\n",
    "- Confidence that the model is good enough in terms of performance.\n",
    "---\n",
    "\n",
    "For details on system level performance benchmarking and GuideLLM, see [Performance Benchmarking with GuideLLM](../docs/System_Level_Performance_Benchmarking.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you did not install dependencies in 02_Base_Performance_Benchmarking/Base.ipynb, uncomment the following line to install dependencies\n",
    "# !pip install -qqU ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from guidellm.benchmark import GenerativeBenchmarksReport\n",
    "from utils import generate, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Check GPU memory\n",
    "\n",
    "To make sure that you have enough GPU memory to run this notebook:\n",
    "\n",
    "1. In a terminal window, run the `nvidia-smi` command.\n",
    "\n",
    "2. If there are processes that are using GPU memory that this notebook requires, run the `kill -9 <pid>` command for each process to stop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Launch an inference server (vLLM) for the compressed model\n",
    "\n",
    "Set up a vLLM inference server to host the compressed model and to expose an OpenAI-compatible API endpoint. This server is required so that GuideLLM can benchmark system-level performance such as throughput, latency, and time-to-first-token. In Module 6, you can compare the performance benchmarks of the base and compressed models.\n",
    "\n",
    "**Resources used** : 46GB L40S GPU x 1\n",
    "\n",
    "For more information about using vLLM, see [Launch Inference Servers for the Base and Compressed Models using vLLM](../docs/Model_Serving_vLLM.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "####  Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level for vLLM inference\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Run the vLLM server\n",
    "\n",
    "This example uses the configuration for a single-node, single-GPU set up to launch a vLLM server for the compressed model. \n",
    "\n",
    "**NOTE:** The configuration for serving the compressed model and the base model (in the [Base.ipynb](../02_Base_Performance_Benchmarking/Base.ipynb) notebook) is similar, differing by the model name and port.\n",
    "\n",
    "1. Open a terminal window.\n",
    "   \n",
    "2. Make sure that the working directory is the `05_Compressed_Performance_Benchmarking` directory.\n",
    "   \n",
    "   `pwd`\n",
    "   \n",
    "3. Run the following command to serve the base model by using vLLM:\n",
    "\n",
    "```bash\n",
    "vllm serve \\\n",
    "  \"../compressed_model/RedHatAI-Llama-3.1-8B-Instruct-int8-dynamic\" \\\n",
    "  --host 127.0.0.1 \\\n",
    "  --port 8001 \\\n",
    "  --gpu-memory-utilization 0.6 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --pipeline-parallel-size 1 \\\n",
    "  --max-model-len 2048\n",
    "```\n",
    "\n",
    "4. Wait for the server to start, as indicated by the following status updates:\n",
    "\n",
    "    ```INFO:     Started server process [166518]```\\\n",
    "    ```INFO:     Waiting for application startup.```\\\n",
    "    ```INFO:     Application startup complete.```\n",
    "\n",
    "**NOTE:** You can safely ignore the following warning message:\n",
    "\n",
    "`The tokenizer you are loading from '../base_model/RedHatAI-Llama-3.1-8B-Instruct' with an incorrect regex pattern... This will lead to incorrect tokenization.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Verify that the vLLM server is accessible\n",
    "\n",
    "Use the  **generate** helper function (defined in [utils.py](./utils.py)) to simplify sending requests to the locally-served vLLM model.\n",
    "\n",
    "This function wraps the OpenAI-compatible Chat Completions API exposed by vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model_path = (\n",
    "    \"../compressed_model/RedHatAI-Llama-3.1-8B-Instruct-int8-dynamic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": [
     "{   \"keep_output\": true }",
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "# For non streaming results\n",
    "response = generate(\n",
    "    model=compressed_model_path,\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8001,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": [
     "{   \"keep_output\": true }"
    ]
   },
   "outputs": [],
   "source": [
    "# For streaming results\n",
    "res = \"\"\n",
    "for chunk in stream(\n",
    "    model=compressed_model_path,\n",
    "    prompt=\"What is photosynthesis?\",\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8001,\n",
    "    api_key=\"empty\",\n",
    "    max_tokens=512,\n",
    "):\n",
    "    res += chunk\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Check GPU vRAM\n",
    "\n",
    "Loading the compressed model with the configuration defined in the previous command uses approximately 28GB. It might seem surprising that a compressed 8.5 GB model consumes approximately 28GB of GPU memory. This consumption is expected for vLLM because of how memory is allocated during inference due to the following contributors:\n",
    "\n",
    "* **Model Weights (~8.5 GB)**\n",
    "\n",
    "    The size of your compressed model stored on disk (INT8, FP16, etc.) which is loaded once into GPU memory.\n",
    "   \n",
    "* **Runtime GPU Memory (~6 GB)**\n",
    "\n",
    "  vLLM reserves extra memory for:\n",
    "\n",
    "   - Parameter sharding.\n",
    "\n",
    "   - CUDA kernels.\n",
    "\n",
    "   - Attention buffers and temporary tensors.\n",
    "\n",
    "   - Weight adapters and padded tensors which adds ~4–8 GB, depending on the model.\n",
    "\n",
    "* **KV Cache (~14 GB)**\n",
    "\n",
    "   - Stores key/value tensors for each generated token to avoid recomputation.\n",
    "\n",
    "   - Memory grows with sequence length, model hidden size, and concurrency.\n",
    "\n",
    "   - vLLM presets a large KV cache to support batching efficiently.\n",
    "\n",
    "\n",
    "* **GPU Memory Utilization Flag (--gpu-memory-utilization)**\n",
    "\n",
    "``--gpu-memory-utilization`` is set to 0.6, which means that vLLM can utilize 60% of the total GPU memory. This example uses one 46GB LS40 GPU. 60% of 46 is approximately 28."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Run performance benchmarking\n",
    "\n",
    "After you verify that the vLLM server for the compressed model is running, you can proceed with benchmarking the compressed model's performance by using **GuideLLM**.\n",
    "\n",
    "1. Identify values for the following parameters:\n",
    "\n",
    "   - **target**: The URL of the vLLM inference server started in the previous step, for example: `http://127.0.0.1:8000` If the vLLM server is running on a different port, update the `target` accordingly.\n",
    "\n",
    "   - **output-path**: The path to save the benchmarking results.\n",
    "\n",
    "2. Open a terminal window.\n",
    "   \n",
    "4. Make sure that the working directory is the `02_Base_Performance_Benchmarking` directory.\n",
    "\n",
    "   `pwd`\n",
    "   \n",
    "6. Check the following command and, if needed, edit the **target** and **output-path** parameters. Then run the command in the terminal.\n",
    "\n",
    "```bash\n",
    "guidellm benchmark \\\n",
    "  --target \"http://127.0.0.1:8001\" \\\n",
    "  --profile sweep \\\n",
    "  --max-seconds 120 \\\n",
    "  --data \"prompt_tokens=1024,output_tokens=512\" \\\n",
    "  --output-path \"../results/compressed_performance_benchmarks.json\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### View the results\n",
    "\n",
    "The result of the `guidellm benchmark` command is formatted in multiple tables. The results display in the terminal and are saved to the directory defined by `output-path`.\n",
    "\n",
    "1. **Request Latency Statistics (Completed Requests)**\n",
    "\n",
    "This table focuses on how **long** requests take and the latency characteristics of the server.\n",
    "\n",
    "```text\n",
    "ℹ Request Latency Statistics (Completed Requests)\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "| Benchmark   | Request Latency || TTFT             || ITL        || TPOT         ||\n",
    "| Strategy    | Sec             || ms               || ms         || ms           ||\n",
    "|             | Mdn     | p95    | Mdn     | p95     | Mdn  | p95  | Mdn   | p95   |\n",
    "|-------------|---------|--------|---------|---------|------|------|-------|-------|\n",
    "| synchronous | 7.6     | 7.9    | 87.9    | 445.7   | 14.7 | 14.7 | 14.8  | 15.5  |\n",
    "| throughput  | 70.4    | 74.8   | 36149.3 | 40360.5 | 63.9 | 99.4 | 137.4 | 146.1 |\n",
    "| constant    | 8.3     | 8.3    | 99.4    | 108.0   | 16.1 | 16.1 | 16.2  | 16.3  |\n",
    "| constant    | 8.9     | 8.9    | 99.1    | 107.2   | 17.2 | 17.3 | 17.4  | 17.4  |\n",
    "| constant    | 9.7     | 9.8    | 104.4   | 113.0   | 18.8 | 18.9 | 19.0  | 19.1  |\n",
    "| constant    | 10.5    | 10.6   | 104.9   | 114.6   | 20.4 | 20.5 | 20.6  | 20.6  |\n",
    "| constant    | 11.7    | 11.8   | 106.9   | 118.1   | 22.7 | 22.8 | 22.8  | 23.0  |\n",
    "| constant    | 12.7    | 12.8   | 108.3   | 119.3   | 24.7 | 24.8 | 24.9  | 24.9  |\n",
    "| constant    | 16.0    | 18.5   | 121.6   | 959.9   | 31.1 | 34.7 | 31.3  | 36.1  |\n",
    "| constant    | 17.8    | 18.1   | 119.7   | 136.0   | 34.5 | 35.2 | 34.7  | 35.4  |\n",
    "|=============|=========|========|=========|=========|======|======|=======|=======|\n",
    "\n",
    "```\n",
    "\n",
    "2.  **Server Throughput Statistics**\n",
    "\n",
    "This table focuses on how many requests a server can handle per second. Throughput can be thought of as the **rate** (or time required) of processing. \n",
    "```text\n",
    "Server Throughput Statistics\n",
    "|=============|=====|======|=======|=======|========|========|=======|========|=======|========|\n",
    "| Benchmark   | Requests                |||| Input Tokens   || Output Tokens || Total Tokens  ||\n",
    "| Strategy    | Per Sec   || Concurrency  || Per Sec        || Per Sec       || Per Sec       ||\n",
    "|             | Mdn | Mean | Mdn   | Mean  | Mdn    | Mean   | Mdn   | Mean   | Mdn   | Mean   |\n",
    "|-------------|-----|------|-------|-------|--------|--------|-------|--------|-------|--------|\n",
    "| synchronous | 0.1 | 0.1  | 1.0   | 1.0   | 139.6  | 148.9  | 68.2  | 67.6   | 68.2  | 207.3  |\n",
    "| throughput  | 0.6 | 2.6  | 194.0 | 152.8 | 123.1  | 4262.7 | 966.7 | 1369.8 | 971.8 | 4200.5 |\n",
    "| constant    | 0.4 | 0.4  | 4.0   | 3.3   | 456.2  | 465.9  | 217.6 | 209.7  | 217.8 | 643.0  |\n",
    "| constant    | 0.7 | 0.7  | 6.0   | 6.1   | 779.7  | 789.8  | 326.7 | 353.9  | 327.1 | 1085.1 |\n",
    "| constant    | 1.0 | 1.0  | 10.0  | 9.3   | 1103.9 | 1113.8 | 422.1 | 495.3  | 422.3 | 1518.7 |\n",
    "| constant    | 1.3 | 1.2  | 14.0  | 12.8  | 1426.6 | 1437.8 | 498.8 | 634.7  | 499.5 | 1946.3 |\n",
    "| constant    | 1.7 | 1.5  | 19.0  | 17.3  | 1753.6 | 1761.9 | 629.9 | 770.0  | 630.6 | 2361.2 |\n",
    "| constant    | 2.0 | 1.7  | 25.0  | 22.0  | 2078.6 | 2085.8 | 746.2 | 901.6  | 747.0 | 2764.8 |\n",
    "| constant    | 2.3 | 2.0  | 36.0  | 32.4  | 2401.2 | 2674.7 | 783.9 | 1110.9 | 786.0 | 3406.4 |\n",
    "| constant    | 2.5 | 2.2  | 44.0  | 37.5  | 2733.0 | 2733.5 | 829.7 | 1123.5 | 831.5 | 3445.2 |\n",
    "|=============|=====|======|=======|=======|========|========|=======|========|=======|========|\n",
    "\n",
    "\n",
    "```\n",
    "#### Compressed Model Performance Summary\n",
    "1. Max concurrency under load: 44.0 (Concurrency Mdn)\n",
    "2. Max output tokens per second under load: 829.7 (Output tokens per sec Mdn)\n",
    "3. Request latency under load: 17.8 (Request Latency in secs Mdn)\n",
    "4. Time to first token under load: 119.7 (TTFT ms Mdn)\n",
    "5. Inter token latency under load: 34.5 (ITL ms Mdn)\n",
    "\n",
    "\n",
    "#### SLO Analysis\n",
    "\n",
    "Assume the Service Level Objective (SLO) is:\n",
    "\n",
    "    TTFT ≤ 200 milliseconds for 95% of requests (p95) with optimal concurrency\n",
    "\n",
    "At the highest tested concurrency of **44 requests**, the compressed model achieves a **p95 TTFT of 136.0 ms**, which comfortably satisfies the SLO.\n",
    "\n",
    "This configuration meets the TTFT SLO of 200 ms for 95% of requests. Increasing concurrency beyond this point may push p95 TTFT above the SLO threshold and should be evaluated carefully in production scenarios.\n",
    "\n",
    "For a workload of **1024 input tokens and 512 output tokens**, the system can sustain approximately **37–44 concurrent requests** while remaining within the TTFT ≤ 200 ms SLO. Reducing input and output token lengths (e.g., 512/256) allows the system to support more concurrent requests while maintaining compliance with the SLO.\n",
    "\n",
    "#### Comparison with Base Model Performance\n",
    "\n",
    "| Metric | Base Model | Compressed Model |\n",
    "|------|-----------|------------------|\n",
    "| p95 TTFT (ms) | 162.4 ms | **136.0 ms** |\n",
    "| Max concurrency under SLO | 34 requests | **44 requests** |\n",
    "| SLO satisfied | Yes | Yes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after the benchmarking process in the terminal completes\n",
    "report = GenerativeBenchmarksReport.load_file(\n",
    "    path=\"../results/compressed_performance_benchmarks.json\",\n",
    ")\n",
    "compressed_benchmarks = report.benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "compressed_benchmarks[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
