{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dec15d-d789-4e22-b9ea-5eec04bf9230",
   "metadata": {},
   "source": [
    "## Evaluate the accuracy of the base model\n",
    "\n",
    "The `Llama-3.1-8B-Instruct` model is the **base model** for the `model-serve-flow` example. Before you compress the model, use this notebook to evaluate the base model on standard benchmarks to establish an accuracy baseline. In Module 6 of this example, you compare this baseline evaluation against the accuracy of the compressed version of the model.\n",
    "\n",
    "**Goal**: Evaluate the base model on standard benchmarks to establish a baseline that you can later compare against the accuracy of the compressed model.\n",
    "\n",
    "**Key actions**:\n",
    "\n",
    "- Test the base model by using the **evaluate** function provided in the `utils.py` file. The  **evaluate** function wraps the LMEval tool's `simple_evaluate` function.\n",
    "\n",
    "- Benchmark on multiple datasets:\n",
    "\n",
    "    - MMLU: General knowledge across subjects.\n",
    "\n",
    "    - IFeval: Instruction-following tasks.\n",
    "\n",
    "    - ARC: Logical and scientific reasoning.\n",
    "    \n",
    "    - HellaSwag: Commonsense completion.\n",
    "\n",
    "- Collect metrics such as accuracy, accuracy_norm, and task-specific scores.\n",
    "\n",
    "- Save results in JSON format.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- Quantitative metrics for the base model.\n",
    "\n",
    "- A baseline to use for comparison against the compressed model's accuracy.\n",
    "\n",
    "For details on evaluating LLMs, see [Evaluate the Accuracy of the Base and Compressed Models](../docs/Accuracy_Evaluation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331ff5e",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070c527e-1caf-4a31-9c83-581938f7ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from lm_eval.utils import make_table\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import evaluate, load_pickle, model_size_gb, save_pickle\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab231b-c501-4151-ad16-0d123d114226",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Check GPU memory\n",
    "\n",
    "To make sure that you have enough GPU memory to run this notebook:\n",
    "\n",
    "1. In a terminal window, run the following command:\n",
    "\n",
    "   `nvidia-smi`\n",
    "\n",
    "2. Verify that the output is similar to the following example:\n",
    "\n",
    "   ```text\n",
    "    +-----------------------------------------------------------------------------------------+\n",
    "    | NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
    "    +-----------------------------------------+------------------------+----------------------+\n",
    "    | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "    | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "    |                                         |                        |               MIG M. |\n",
    "    |=========================================+========================+======================|\n",
    "    |   0  NVIDIA L40S                    On  |   00000000:CA:00.0 Off |                    0 |\n",
    "    | N/A   44C    P0             91W /  350W |   15753MiB /  46068MiB |      0%      Default |\n",
    "    |                                         |                        |                  N/A |\n",
    "    +-----------------------------------------+------------------------+----------------------+\n",
    "\n",
    "    +-----------------------------------------------------------------------------------------+\n",
    "    | Processes:                                                                              |\n",
    "    |  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "    |        ID   ID                                                               Usage      |\n",
    "    |=========================================================================================|\n",
    "    |    0   N/A  N/A            8049      C   /opt/app-root/bin/python3             15744MiB |\n",
    "    +-----------------------------------------------------------------------------------------+\n",
    "\n",
    "\n",
    "\n",
    "3. If there are processes that are using GPU memory that this notebook requires to run, stop the processes:\n",
    " \n",
    "   a. Note the PID number for each process.\n",
    "\n",
    "   b. Run the `kill -9 <pid>` command for each process to stop it.\n",
    "\n",
    "      For example, if the PID number is  `8049`, run the following command:\n",
    "\n",
    "        ```\n",
    "        `kill -9 8049`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd9a14",
   "metadata": {},
   "source": [
    "### Load the base model\n",
    "\n",
    "For this example, the `RedHatAI/Llama-3.1-8B-Instruct` model is the base model, as defined in the `model_name` variable.\n",
    "To load the model in the data type specified in its configuration, load the model with **from_pretrained** using the **AutoModelForCausalLM** class. Specify the data type by setting the **torch_dtype** parameter to **auto**. Otherwise, PyTorch loads the weights in **full precision (fp32)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "model_name = \"RedHatAI/Llama-3.1-8B-Instruct\"\n",
    "base_model_path = f\"../base_model/{model_name.replace('/', '-')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and tokenizer from huggingfaceabs\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "model.config.dtype = \"bfloat16\"\n",
    "# saving model and tokenizer\n",
    "model.save_pretrained(base_model_path)\n",
    "tokenizer.save_pretrained(base_model_path)\n",
    "\n",
    "print(\"Base model saved at:\", base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model size\n",
    "# !du -sh {base_model_path}\n",
    "model_size = model_size_gb(model)\n",
    "print(f\"The size of the base model is: {model_size:.4f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593bc10b",
   "metadata": {},
   "source": [
    "Note that the base model (`Llama-3.1-8B-Instruct`) is approximately 15GB in size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfb397-d989-4a4b-8636-511a25c12ed6",
   "metadata": {},
   "source": [
    "### Define evaluation benchmarking datasets\n",
    "\n",
    "Use the following benchmark datasets to evaluate multiple tasks:\n",
    "- MMLU: General knowledge across 57 subjects\n",
    "- IFeval: Instruction-following capability\n",
    "- ARC: Logical and scientific reasoning\n",
    "- HellaSwag: Commonsense completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383befc1-e7b7-4540-af75-a3d5ce28786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tasks you want to evaluate the model on\n",
    "tasks = [\"mmlu\", \"arc_easy\", \"hellaswag\", \"ifeval\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddc52-4acd-4230-9681-f84201f19e33",
   "metadata": {},
   "source": [
    "### Evaluate the base model\n",
    "\n",
    "**NOTE**: \n",
    "- Running the evaluation on the entire list of tasks can take a long time (5 hours or more depending on resources). For the purpose of testing, run the evaluation on a single task instead.\n",
    "\n",
    "- The results are stored as a **results.pkl** file in the directory defined by **base_results_dir**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b759ca-440b-4491-abba-486fe2c1afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories\n",
    "base_results_dir = \"../results/base_accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71d935",
   "metadata": {},
   "source": [
    "**NOTE:** If the following warning appears when you run the next cell, you can safely ignore it:\n",
    "\n",
    "`The tokenizer you are loading from '../base_model' with an incorrect regex pattern... This will lead to incorrect tokenization.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff8288-c452-4d04-89cc-ddf57339469b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluate the base model and save results in pkl format\n",
    "base_acc = evaluate(\n",
    "    base_model_path,\n",
    "    tasks,\n",
    "    limit=None,\n",
    "    batch_size=\"auto\",\n",
    "    apply_chat_template=True,\n",
    "    verbosity=None,\n",
    ")\n",
    "save_pickle(base_results_dir, base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e51ac-f4c3-4276-b2fe-df595a963443",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = load_pickle(base_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36511cfb-fac8-4f8c-be17-c2d98ffb8560",
   "metadata": {
    "tags": [
     "parameters",
     "{   \"keep_output\": true }"
    ]
   },
   "outputs": [],
   "source": [
    "# print results for the base model\n",
    "print(make_table(base_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d8b67",
   "metadata": {},
   "source": [
    "**Example Accuracy results for the base model:**\n",
    "\n",
    "\n",
    "```text\n",
    "|                 Tasks                 |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|\n",
    "|---------------------------------------|------:|------|-----:|-----------------------|---|-----:|---|------|\n",
    "|arc_easy                               |      1|none  |     0|acc                    |↑  |0.8136|±  |0.0080|\n",
    "|                                       |       |none  |     0|acc_norm               |↑  |0.7588|±  |0.0088|\n",
    "|hellaswag                              |      1|none  |     0|acc                    |↑  |0.5741|±  |0.0049|\n",
    "|                                       |       |none  |     0|acc_norm               |↑  |0.7251|±  |0.0045|\n",
    "|ifeval                                 |      4|none  |     0|inst_level_loose_acc   |↑  |0.8513|±  |   N/A|\n",
    "|                                       |       |none  |     0|inst_level_strict_acc  |↑  |0.8189|±  |   N/A|\n",
    "|                                       |       |none  |     0|prompt_level_loose_acc |↑  |0.7874|±  |0.0176|\n",
    "|                                       |       |none  |     0|prompt_level_strict_acc|↑  |0.7449|±  |0.0188|\n",
    "|mmlu                                   |      2|none  |      |acc                    |↑  |0.6322|±  |0.0038|\n",
    "| - humanities                          |      2|none  |      |acc                    |↑  |0.5864|±  |0.0068|\n",
    "|  - formal_logic                       |      1|none  |     0|acc                    |↑  |0.4921|±  |0.0447|\n",
    "|  - high_school_european_history       |      1|none  |     0|acc                    |↑  |0.7455|±  |0.0340|\n",
    "|  - high_school_us_history             |      1|none  |     0|acc                    |↑  |0.7892|±  |0.0286|\n",
    "|  - high_school_world_history          |      1|none  |     0|acc                    |↑  |0.8186|±  |0.0251|\n",
    "|  - international_law                  |      1|none  |     0|acc                    |↑  |0.7686|±  |0.0385|\n",
    "|  - jurisprudence                      |      1|none  |     0|acc                    |↑  |0.7315|±  |0.0428|\n",
    "|  - logical_fallacies                  |      1|none  |     0|acc                    |↑  |0.7730|±  |0.0329|\n",
    "|  - moral_disputes                     |      1|none  |     0|acc                    |↑  |0.6792|±  |0.0251|\n",
    "|  - moral_scenarios                    |      1|none  |     0|acc                    |↑  |0.4179|±  |0.0165|\n",
    "|  - philosophy                         |      1|none  |     0|acc                    |↑  |0.6977|±  |0.0261|\n",
    "|  - prehistory                         |      1|none  |     0|acc                    |↑  |0.7130|±  |0.0252|\n",
    "|  - professional_law                   |      1|none  |     0|acc                    |↑  |0.4687|±  |0.0127|\n",
    "|  - world_religions                    |      1|none  |     0|acc                    |↑  |0.8480|±  |0.0275|\n",
    "| - other                               |      2|none  |      |acc                    |↑  |0.7184|±  |0.0078|\n",
    "|  - business_ethics                    |      1|none  |     0|acc                    |↑  |0.6500|±  |0.0479|\n",
    "|  - clinical_knowledge                 |      1|none  |     0|acc                    |↑  |0.7094|±  |0.0279|\n",
    "|  - college_medicine                   |      1|none  |     0|acc                    |↑  |0.6416|±  |0.0366|\n",
    "|  - global_facts                       |      1|none  |     0|acc                    |↑  |0.4200|±  |0.0496|\n",
    "|  - human_aging                        |      1|none  |     0|acc                    |↑  |0.6771|±  |0.0314|\n",
    "|  - management                         |      1|none  |     0|acc                    |↑  |0.8058|±  |0.0392|\n",
    "|  - marketing                          |      1|none  |     0|acc                    |↑  |0.8547|±  |0.0231|\n",
    "|  - medical_genetics                   |      1|none  |     0|acc                    |↑  |0.8100|±  |0.0394|\n",
    "|  - miscellaneous                      |      1|none  |     0|acc                    |↑  |0.8084|±  |0.0141|\n",
    "|  - nutrition                          |      1|none  |     0|acc                    |↑  |0.7549|±  |0.0246|\n",
    "|  - professional_accounting            |      1|none  |     0|acc                    |↑  |0.5177|±  |0.0298|\n",
    "|  - professional_medicine              |      1|none  |     0|acc                    |↑  |0.7610|±  |0.0259|\n",
    "|  - virology                           |      1|none  |     0|acc                    |↑  |0.5663|±  |0.0386|\n",
    "| - social sciences                     |      2|none  |      |acc                    |↑  |0.7442|±  |0.0077|\n",
    "|  - econometrics                       |      1|none  |     0|acc                    |↑  |0.4386|±  |0.0467|\n",
    "|  - high_school_geography              |      1|none  |     0|acc                    |↑  |0.7929|±  |0.0289|\n",
    "|  - high_school_government_and_politics|      1|none  |     0|acc                    |↑  |0.8497|±  |0.0258|\n",
    "|  - high_school_macroeconomics         |      1|none  |     0|acc                    |↑  |0.6564|±  |0.0241|\n",
    "|  - high_school_microeconomics         |      1|none  |     0|acc                    |↑  |0.7479|±  |0.0282|\n",
    "|  - high_school_psychology             |      1|none  |     0|acc                    |↑  |0.8642|±  |0.0147|\n",
    "|  - human_sexuality                    |      1|none  |     0|acc                    |↑  |0.7634|±  |0.0373|\n",
    "|  - professional_psychology            |      1|none  |     0|acc                    |↑  |0.6797|±  |0.0189|\n",
    "|  - public_relations                   |      1|none  |     0|acc                    |↑  |0.6909|±  |0.0443|\n",
    "|  - security_studies                   |      1|none  |     0|acc                    |↑  |0.6898|±  |0.0296|\n",
    "|  - sociology                          |      1|none  |     0|acc                    |↑  |0.8308|±  |0.0265|\n",
    "|  - us_foreign_policy                  |      1|none  |     0|acc                    |↑  |0.8600|±  |0.0349|\n",
    "| - stem                                |      2|none  |      |acc                    |↑  |0.5062|±  |0.0084|\n",
    "|  - abstract_algebra                   |      1|none  |     0|acc                    |↑  |0.2700|±  |0.0446|\n",
    "|  - anatomy                            |      1|none  |     0|acc                    |↑  |0.6222|±  |0.0419|\n",
    "|  - astronomy                          |      1|none  |     0|acc                    |↑  |0.6974|±  |0.0374|\n",
    "|  - college_biology                    |      1|none  |     0|acc                    |↑  |0.7708|±  |0.0351|\n",
    "|  - college_chemistry                  |      1|none  |     0|acc                    |↑  |0.4700|±  |0.0502|\n",
    "|  - college_computer_science           |      1|none  |     0|acc                    |↑  |0.4100|±  |0.0494|\n",
    "|  - college_mathematics                |      1|none  |     0|acc                    |↑  |0.2800|±  |0.0451|\n",
    "|  - college_physics                    |      1|none  |     0|acc                    |↑  |0.3922|±  |0.0486|\n",
    "|  - computer_security                  |      1|none  |     0|acc                    |↑  |0.7200|±  |0.0451|\n",
    "|  - conceptual_physics                 |      1|none  |     0|acc                    |↑  |0.5915|±  |0.0321|\n",
    "|  - electrical_engineering             |      1|none  |     0|acc                    |↑  |0.5724|±  |0.0412|\n",
    "|  - elementary_mathematics             |      1|none  |     0|acc                    |↑  |0.3942|±  |0.0252|\n",
    "|  - high_school_biology                |      1|none  |     0|acc                    |↑  |0.7581|±  |0.0244|\n",
    "|  - high_school_chemistry              |      1|none  |     0|acc                    |↑  |0.5123|±  |0.0352|\n",
    "|  - high_school_computer_science       |      1|none  |     0|acc                    |↑  |0.6100|±  |0.0490|\n",
    "|  - high_school_mathematics            |      1|none  |     0|acc                    |↑  |0.2481|±  |0.0263|\n",
    "|  - high_school_physics                |      1|none  |     0|acc                    |↑  |0.3709|±  |0.0394|\n",
    "|  - high_school_statistics             |      1|none  |     0|acc                    |↑  |0.4213|±  |0.0337|\n",
    "|  - machine_learning                   |      1|none  |     0|acc                    |↑  |0.4911|±  |0.0475|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
