{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7375209-d552-4543-9e7f-1c9a03117314",
   "metadata": {},
   "source": [
    "## Compress the base model by using LLM compressor\n",
    "\n",
    "After you establish **accuracy and performance baselines** for the base model, you can compress it. This notebook uses the quantization technique. The purpose of quantitizing the model is to reduce the modelâ€™s memory footprint, resulting in a smaller and more deployment-efficient model.\n",
    "\n",
    "**Note**: This notebook uses **data-aware quantization**, which relies on representative calibration data to preserve model quality.\n",
    "\n",
    "**Goal**: Reduce the model size while retaining its accuracy and inference performance.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "- Load the base model.\n",
    "\n",
    "- Measure its size and memory usage.\n",
    "\n",
    "- Use a calibration dataset (WikiText, UltraChat) to collect activation statistics.\n",
    "\n",
    "- Apply a quantization recipe (SmoothQuant + GPTQ modifier).\n",
    "\n",
    "- Save the compressed model and verify the reduction in its size.\n",
    "\n",
    "**Outcome**:\n",
    "\n",
    "- The compressed model saved on disk.\n",
    "\n",
    "- The model size is reduced, typically by 50% (depending on quantization scheme)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69ebf9",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22405c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqU .\n",
    "!pip install -qqU torch==2.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0aac4-4079-41af-a087-ac52602d1782",
   "metadata": {},
   "source": [
    "***NOTE:** You can safely ignore the following error:  \n",
    "\n",
    "`ERROR: pip's dependency resolver does not currently take into account... llmcompressor 0.8.1 requires torch<=2.8.0,>=2.7.0, but you have torch 2.9.0 which is incompatible.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d76ec6-9bab-4a4b-bda8-0e5663510937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils import model_size_gb, tokenize_for_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb81706-2da2-4db7-8132-f2d12d82e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53695b-a3ce-49a2-9f91-7c335e948a08",
   "metadata": {},
   "source": [
    "### Check GPU memory\n",
    "\n",
    "To make sure that you have enough GPU memory to run this notebook:\n",
    "\n",
    "1. In a terminal window, run the `nvidia-smi` command.\n",
    "\n",
    "2.  If there are processes that are using GPU memory that this notebook requires, run the `kill -9 <pid>` command for each process to stop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6585cb-614e-403b-beb1-36a9c03f66b8",
   "metadata": {},
   "source": [
    "### Load the base model\n",
    "\n",
    "To load the model in the data type specified in its configuration, load the model with **from_pretrained** using the **AutoModelForCausalLM** class. Specify the data type by setting the **torch_dtype** parameter to **auto**. Otherwise, PyTorch loads the weights in **full precision (fp32)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799031ae-0932-4924-86e6-68d7917c482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "base_model_path = \"../base_model/RedHatAI-Llama-3.1-8B-Instruct\"\n",
    "compressed_model_path = (\n",
    "    \"../compressed_model/RedHatAI-Llama-3.1-8B-Instruct-int8-dynamic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197428d9-a98d-478b-8e04-dbd3c9dad8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and tokenizer from huggingfaceabs\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "print(\"Base model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813fe3e0-e35d-4786-b099-6181c1c0243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model size\n",
    "# !du -sh {base_model_path}\n",
    "model_size = model_size_gb(model)\n",
    "print(f\"The size of the base model is: {model_size:.4f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b8093-b984-4d58-a552-65692dc7944c",
   "metadata": {},
   "source": [
    "### Prepare a calibration dataset\n",
    "\n",
    "Because this example uses data-aware quantization to compress the base model, you must use a dataset to calibrate the model with real or representative inputs. For this example, use the `wikitext-2-raw-v1` version of the WikiText dataset. It is a small, general-purpose dataset for faster processing. For  information about calibration datasets, see [Compress a Large Language Model by using LLM Compressor](../docs/Compression.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dd2e2-58d1-47e8-b17b-2a0d78627e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset to use for calibration\n",
    "dataset_id = \"wikitext\"\n",
    "\n",
    "# Specify the configuration / version of the dataset\n",
    "config = \"wikitext-2-raw-v1\"  # Small version (~2M tokens), raw text format\n",
    "\n",
    "# Set the number of calibration samples based on available device\n",
    "# - On GPU: use more samples to get more accurate activation statistics\n",
    "# - On CPU: reduce samples to prevent memory issues and keep demo fast\n",
    "num_calibration_samples = 512 if device == \"cuda\" else 16\n",
    "\n",
    "# Set the maximum sequence length for calibration\n",
    "max_sequence_length = 1024 if device == \"cuda\" else 16\n",
    "\n",
    "# Load the dataset using Hugging Face Datasets API\n",
    "# This downloads train split of the dataset\n",
    "ds = load_dataset(dataset_id, config, split=\"train\")\n",
    "# Shuffle and grab only the number of samples we need\n",
    "ds = ds.shuffle(seed=42).select(range(num_calibration_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e60f8-6420-41aa-91da-266b2ce828c1",
   "metadata": {},
   "source": [
    "Inspect the dataset by extracting the contents of the columns to determine which column contains the content that you want to use. For the `wikitext-2-raw-v1` dataset in this example, the `text` column contains the content that is passed as input for calibrating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bbf91-a2a5-4728-a722-05a6f87a3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the dataset\n",
    "print(f\"columns in the {dataset_id}: {ds.column_names}\\n\")\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf853df6-2fbd-4a95-ae92-52c7b11577a4",
   "metadata": {},
   "source": [
    "### (Optional) Use a custom template for calibration\n",
    "\n",
    "Use a **custom template** when you want the calibration text to closely mimic the input format that the model will encounter in production.  \n",
    "\n",
    "For example, if your model is **instruction-following** or **chat-based**, by providing the template the model was originally trained on or the template that will be used during inference, you can ensure that the activation statistics collected during calibration reflect realistic usage patterns. \n",
    "\n",
    "By using a custom template for calibration, you can improve the accuracy of quantization and compression.\n",
    "\n",
    "If your model can handle raw text and does not require a specific format, you can rely on the default template instead.\n",
    "\n",
    "To specify a custom template, use the `tokenize_for_calibration` function with the `custom_template` argument. The `custom_template` argument accepts the following format:\n",
    "\n",
    "```python\n",
    "custom_template = {\n",
    " \"template_text\": \"Instruction: {content}\\nOutput:\", \n",
    " \"placeholder\": \"content\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54934efc-9564-446c-a37b-daf58dcb5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get activations for the calibration dataset, we need to:\n",
    "# 1. extract the samples from the dataset\n",
    "# 2. tokenize samples in the dataset\n",
    "input_column = \"text\"\n",
    "\n",
    "# Call tokenize_for_calibration using dataset.map\n",
    "tokenized_dataset = ds.map(\n",
    "    lambda batch: tokenize_for_calibration(\n",
    "        examples=batch,  # batch from Hugging Face dataset\n",
    "        input_column=input_column,  # the column containing text to calibrate\n",
    "        tokenizer=tokenizer,  # your Hugging Face tokenizer\n",
    "        max_length=max_sequence_length,  # maximum sequence length\n",
    "        model_type=\"chat\",  # use chat template if no custom template\n",
    "        custom_template=None,  # optional, provide a dict if you want a custom template\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afffdb-86da-401b-a1d9-87d8eeef3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5357d71-6eed-40fa-972b-d7e428422414",
   "metadata": {},
   "source": [
    "### Quantize the base model to INT8\n",
    "\n",
    "After preparing the dataset for calibration, define a recipe for quantization. For quantization scheme `W8A8-INT8`, use `SmoothQuantModifier` followed by `GPTQModifier`.\n",
    "\n",
    "For details on the SmoothQuant and GPTQ algorithms, see [Compress a Large Language Model by using LLM Compressor](Compression.md).\n",
    "\n",
    "**NOTE:** The process of running the next cell to compress the model might take a significant amount of time (approximately one hour), as the model is executed on a calibration dataset to collect and calibrate activation statistics prior to applying quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df95b69-40b7-49ed-a537-76299c916c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the quantization scheme\n",
    "scheme = \"W8A8\"  # W8A8 means 8-bit weights and 8-bit activations\n",
    "\n",
    "# Strength for SmoothQuant smoothing\n",
    "# This controls how much the activation values are smoothed to reduce outliers\n",
    "smoothing_strength = 0.8\n",
    "\n",
    "# Create SmoothQuant modifier\n",
    "# - smooths activations before quantization to improve stability and reduce degradation\n",
    "smooth_quant = SmoothQuantModifier(smoothing_strength=smoothing_strength)\n",
    "\n",
    "# Create GPTQ modifier\n",
    "# - targets=\"Linear\" quantizes only Linear layers (e.g., feedforward layers)\n",
    "# - scheme=scheme uses the W8A8 quantization scheme\n",
    "# - ignore=[\"lm_head\"] preserves the LM head to avoid generation quality loss\n",
    "quantizer = GPTQModifier(targets=\"Linear\", scheme=scheme, ignore=[\"lm_head\"])\n",
    "\n",
    "# Combine the modifiers into a recipe list\n",
    "# The order matters: first apply SmoothQuant, then GPTQ\n",
    "recipe = [smooth_quant, quantizer]\n",
    "\n",
    "# Perform quantization\n",
    "oneshot(\n",
    "    model=base_model_path,  # Model to quantize\n",
    "    dataset=tokenized_dataset,  # Calibration dataset, used for both SmoothQuant & GPTQ\n",
    "    recipe=recipe,  # List of quantization modifiers to apply\n",
    "    output_dir=compressed_model_path,  # Directory to save the quantized model\n",
    "    max_seq_length=2048,  # Maximum sequence length for calibration\n",
    "    num_calibration_samples=512,  # Number of samples used for calibration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf956831",
   "metadata": {},
   "source": [
    "### Check the size of the compressed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82cf8c-2e48-4939-aee7-c61beb75456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load quantized model\n",
    "model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "model_quant.config.dtype = model.config.torch_dtype\n",
    "model_quant.save_pretrained(compressed_model_path)\n",
    "model_size = model_size_gb(model_quant)\n",
    "print(f\"Model size (GB): {model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882026b-2c4c-4e1f-b20f-75fdaf71b7f3",
   "metadata": {},
   "source": [
    "After quantizing the model, the size reduces from 14.9GB to 8GB. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0697fb-730d-442c-8013-09030881518f",
   "metadata": {},
   "source": [
    "### Alternate compression technique: FP8 quantization\n",
    "\n",
    "The LLM compressor also supports FP8 quantization. This conversion does not require any calibration dataset. To quantize the model to FP8, uncomment and then run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c84a3-83b0-4d28-9665-13e4f1ef5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe = QuantizationModifier(\n",
    "#   targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n",
    "\n",
    "# oneshot(model=model_name, recipe=recipe, output_dir=compressed_model_path)\n",
    "\n",
    "# # Load quantized model\n",
    "# model_quant = AutoModelForCausalLM.from_pretrained(compressed_model_path)\n",
    "# model_quant.config.dtype = model.config.torch_dtype\n",
    "# model_quant.save_pretrained(compressed_model_path)\n",
    "# model_size = model_size_gb(model_quant)\n",
    "# print(f\"Model size (GB): {model_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
