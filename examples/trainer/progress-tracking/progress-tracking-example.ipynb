{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155e8874",
   "metadata": {},
   "source": [
    "# Real-Time Progress Tracking with TransformersTrainer\n",
    "\n",
    "This notebook demonstrates how to monitor distributed training progress in real-time using `TransformersTrainer` from Kubeflow Trainer v2 on Red Hat OpenShift AI.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this example, we fine-tune the **Qwen 2.5 1.5B Instruct** model on the **Stanford Alpaca** instruction-following dataset. The training runs on 2 GPU nodes with automatic progress tracking enabled, allowing you to monitor training metrics in real-time from the OpenShift AI Dashboard.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Automatic Progress Tracking** | TransformersTrainer auto-injects a `KubeflowProgressCallback` that exposes training metrics via HTTP |\n",
    "| **Real-Time Metrics** | View current step, epoch, loss, and estimated time remaining in the OpenShift AI Dashboard |\n",
    "| **PVC-Based Checkpointing** | Save model checkpoints to a shared PersistentVolumeClaim for durability and resume capability |\n",
    "| **Distributed Training** | Run training across multiple GPU nodes using PyTorch's DistributedDataParallel (DDP) |\n",
    "\n",
    "### Model Details\n",
    "\n",
    "**Qwen 2.5 1.5B Instruct** is a compact instruction-tuned language model from the Qwen family:\n",
    "- **Parameters:** 1.5 billion\n",
    "- **Context Length:** 32K tokens\n",
    "- **Languages:** Multilingual with strong English and Chinese support\n",
    "- **Use Case:** Ideal for instruction-following, chat, and text generation tasks\n",
    "- **Why this model?** Small enough to train quickly for demonstration, yet powerful enough for real-world tasks\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **OpenShift AI Cluster** with Kubeflow Trainer v2 enabled\n",
    "2. **Workbench** running Python 3.12+ with GPU access\n",
    "3. **Environment Variables:**\n",
    "   - `OPENSHIFT_API_URL` - Your OpenShift API server URL\n",
    "   - `NOTEBOOK_USER_TOKEN` - Authentication token for API access\n",
    "4. **Shared PVC** named `shared` mounted at `/opt/app-root/src/shared` in the workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306a076",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install the Kubeflow SDK and required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c80093",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install datasets transformers accelerate huggingface_hub\n",
    "!python3 -m pip install --force-reinstall --no-cache-dir -U \"kubeflow @ git+https://github.com/opendatahub-io/kubeflow-sdk.git@v0.2.1+rhai0\"\n",
    "!python3 -m pip install --force-reinstall --no-cache-dir -U ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b68cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kubeflow\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.rhai import TransformersTrainer\n",
    "from kubernetes import client as k8s\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f\"Kubeflow SDK version: {kubeflow.__version__}\")\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e13d5f",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configure authentication and paths. TransformersTrainer has **progress tracking enabled by default**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication configuration\n",
    "api_server = os.getenv(\"OPENSHIFT_API_URL\")\n",
    "token = os.getenv(\"NOTEBOOK_USER_TOKEN\")\n",
    "\n",
    "if not api_server or not token:\n",
    "    raise RuntimeError(\n",
    "        \"OPENSHIFT_API_URL and NOTEBOOK_USER_TOKEN environment variables are required\"\n",
    "    )\n",
    "\n",
    "# Configure Kubernetes client\n",
    "configuration = k8s.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.verify_ssl = False  # Set to True if using trusted certificates\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# PVC Configuration\n",
    "PVC_NAME = \"shared\"\n",
    "# In the notebook, the shared PVC is mounted at /opt/app-root/src/shared\n",
    "# In training pods, we mount it at /opt/app-root/src\n",
    "NOTEBOOK_SHARED_PATH = \"/opt/app-root/src/shared\"  # Where notebook sees the shared PVC\n",
    "TRAINING_POD_PATH = \"/opt/app-root/src\"  # Where training pods will mount it\n",
    "\n",
    "# Model Configuration - use notebook path for downloading, training path for train_func\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# Paths for notebook (downloading)\n",
    "MODEL_PATH = f\"{NOTEBOOK_SHARED_PATH}/models/qwen2.5-1.5b-instruct\"\n",
    "DATA_PATH = f\"{NOTEBOOK_SHARED_PATH}/data/alpaca_processed\"\n",
    "CHECKPOINTS_PATH = f\"{NOTEBOOK_SHARED_PATH}/checkpoints/progress-tracking\"\n",
    "\n",
    "# Paths for training pods (used in train_func and PodTemplateOverrides)\n",
    "TRAINING_MODEL_PATH = f\"{TRAINING_POD_PATH}/models/qwen2.5-1.5b-instruct\"\n",
    "TRAINING_DATA_PATH = f\"{TRAINING_POD_PATH}/data/alpaca_processed\"\n",
    "TRAINING_CHECKPOINTS_PATH = f\"{TRAINING_POD_PATH}/checkpoints/progress-tracking\"\n",
    "\n",
    "print(f\"API Server: {api_server}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Notebook Model Path: {MODEL_PATH}\")\n",
    "print(f\"Training Pod Model Path: {TRAINING_MODEL_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")\n",
    "print(f\"Checkpoints Path: {CHECKPOINTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71d2ba",
   "metadata": {},
   "source": [
    "## Download Model and Dataset to Shared PVC\n",
    "\n",
    "Before submitting the training job, we pre-download the model and dataset to the shared PVC. This ensures:\n",
    "- **Offline Training:** Training pods don't need internet access during training\n",
    "- **Faster Startup:** No download delays when training pods start\n",
    "- **Consistency:** All nodes use the same model weights and data\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "We use the **Stanford Alpaca** dataset (`tatsu-lab/alpaca`), a widely-used instruction-following dataset:\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Source** | Stanford University |\n",
    "| **Size** | 52,000 instruction-response pairs |\n",
    "| **Format** | Instruction, optional input, and response |\n",
    "| **Use Case** | Instruction-tuning language models |\n",
    "\n",
    "Each sample follows this structure:\n",
    "```\n",
    "### Instruction:\n",
    "Give three tips for staying healthy.\n",
    "\n",
    "### Response:\n",
    "1. Eat a balanced diet...\n",
    "2. Exercise regularly...\n",
    "3. Get enough sleep...\n",
    "```\n",
    "\n",
    "For this demo, we use a **500-sample subset** to enable quick training (~1 minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model to PVC\n",
    "if os.path.exists(MODEL_PATH) and os.listdir(MODEL_PATH):\n",
    "    print(f\"‚úÖ Model already exists at {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"üîÑ Downloading model {MODEL_NAME} to {MODEL_PATH}...\")\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "    # Use fast tokenizer for compatibility\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.save_pretrained(MODEL_PATH, safe_serialization=True)\n",
    "    print(f\"‚úÖ Model saved to {MODEL_PATH}\")\n",
    "    print(f\"üìÅ Files: {os.listdir(MODEL_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "if os.path.exists(DATA_PATH) and os.listdir(DATA_PATH):\n",
    "    print(f\"‚úÖ Dataset already exists at {DATA_PATH}\")\n",
    "else:\n",
    "    print(\"üîÑ Downloading and processing Alpaca dataset...\")\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "    # Load subset of Alpaca dataset\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
    "\n",
    "    # Load tokenizer for preprocessing\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def format_instruction(example):\n",
    "        if example.get(\"input\"):\n",
    "            text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        else:\n",
    "            text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        return {\"text\": text}\n",
    "\n",
    "    dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        )\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    "    )\n",
    "    tokenized_dataset.save_to_disk(DATA_PATH)\n",
    "    print(f\"‚úÖ Dataset saved to {DATA_PATH}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model and dataset ready on PVC!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db38f5",
   "metadata": {},
   "source": [
    "## Define the Training Function\n",
    "\n",
    "The training function runs inside each training pod as a distributed PyTorch process. TransformersTrainer serializes this function and executes it via `torchrun` on each node.\n",
    "\n",
    "### How Progress Tracking Works\n",
    "\n",
    "When you use `TransformersTrainer` with `enable_progression_tracking=True` (the default):\n",
    "\n",
    "1. **Automatic Instrumentation:** TransformersTrainer injects a `KubeflowProgressCallback` into your HuggingFace `Trainer`\n",
    "2. **HTTP Metrics Server:** A lightweight HTTP server starts on port 28080, exposing metrics as JSON\n",
    "3. **Dashboard Integration:** OpenShift AI Dashboard polls these metrics and displays real-time progress\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `num_train_epochs` | 1 | Complete one pass through the dataset |\n",
    "| `per_device_train_batch_size` | 2 | Samples per GPU per step |\n",
    "| `gradient_accumulation_steps` | 4 | Effective batch size = 2 √ó 4 √ó 2 nodes = 16 |\n",
    "| `learning_rate` | 2e-5 | Standard fine-tuning rate |\n",
    "| `save_steps` | 20 | Checkpoint every 20 steps |\n",
    "| `bf16` | True | Use bfloat16 mixed precision |\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Supported Trainers:** Use `transformers.Trainer` or `trl.SFTTrainer` - both are auto-instrumented\n",
    "- **No Manual Setup:** Progress tracking callback is injected automatically\n",
    "- **Local Files Only:** Model and data are loaded from the mounted PVC (no network access needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0feb354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    \"\"\"SFT training function using HuggingFace Trainer.\n",
    "\n",
    "    TransformersTrainer automatically:\n",
    "    - Injects KubeflowProgressCallback for real-time metrics\n",
    "    - Applies checkpoint configuration from periodic_checkpoint_config\n",
    "    - Enables auto-resume from latest checkpoint\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # IMPORTANT: Set offline mode BEFORE importing transformers/huggingface_hub\n",
    "    # This prevents the newer huggingface_hub from validating local paths as repo IDs\n",
    "    os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "    os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "\n",
    "    import torch\n",
    "    from datasets import load_from_disk\n",
    "    from transformers import (\n",
    "        AutoConfig,\n",
    "        AutoModelForCausalLM,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        PreTrainedTokenizerFast,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "\n",
    "    rank = int(os.environ.get(\"RANK\", 0))\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "\n",
    "    # Paths on the shared PVC (mounted via pvc:// URI)\n",
    "    model_path = \"/opt/app-root/src/models/qwen2.5-1.5b-instruct\"\n",
    "    data_path = \"/opt/app-root/src/data/alpaca_processed\"\n",
    "    # output_dir is set by TransformersTrainer from the output_dir parameter\n",
    "    output_dir = \"/opt/app-root/src/checkpoints/progress-tracking\"\n",
    "\n",
    "    print(f\"üöÄ Starting training on rank {rank}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        print(f\"üîß GPU: {torch.cuda.get_device_name(local_rank)}\")\n",
    "\n",
    "    # Load tokenizer directly from tokenizer.json file\n",
    "    # This bypasses AutoTokenizer's hub validation that fails with local paths\n",
    "    print(f\"üì• Loading tokenizer from: {model_path}\")\n",
    "    tokenizer_file = os.path.join(model_path, \"tokenizer.json\")\n",
    "    tokenizer_config_file = os.path.join(model_path, \"tokenizer_config.json\")\n",
    "\n",
    "    # Load tokenizer config to get special tokens\n",
    "    import json\n",
    "\n",
    "    with open(tokenizer_config_file) as f:\n",
    "        tokenizer_config = json.load(f)\n",
    "\n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_file=tokenizer_file,\n",
    "        eos_token=tokenizer_config.get(\"eos_token\", \"<|endoftext|>\"),\n",
    "        pad_token=tokenizer_config.get(\"pad_token\"),\n",
    "        bos_token=tokenizer_config.get(\"bos_token\"),\n",
    "        unk_token=tokenizer_config.get(\"unk_token\"),\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model config first, then model - avoids hub validation issues\n",
    "    print(f\"üì• Loading model from: {model_path}\")\n",
    "    config_file = os.path.join(model_path, \"config.json\")\n",
    "    with open(config_file) as f:\n",
    "        model_config_dict = json.load(f)\n",
    "\n",
    "    # Extract model_type and pass remaining config\n",
    "    model_type = model_config_dict.pop(\"model_type\")\n",
    "    config = AutoConfig.for_model(model_type, **model_config_dict)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": local_rank},\n",
    "        local_files_only=True,\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"üì• Loading dataset from: {data_path}\")\n",
    "    tokenized_dataset = load_from_disk(data_path)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Training arguments - TransformersTrainer will override save_* settings\n",
    "    # from periodic_checkpoint_config if provided\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        bf16=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "\n",
    "    # Trainer - TransformersTrainer automatically injects KubeflowProgressCallback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train - auto-resumes from latest checkpoint if available\n",
    "    trainer.train()\n",
    "\n",
    "    # Save final model (only on rank 0)\n",
    "    if rank == 0:\n",
    "        final_path = f\"{output_dir}/final\"\n",
    "        trainer.save_model(final_path)\n",
    "        tokenizer.save_pretrained(final_path)\n",
    "        print(f\"‚úÖ Final model saved to {final_path}\")\n",
    "\n",
    "    print(f\"‚úÖ Training complete on rank {rank}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ef44d",
   "metadata": {},
   "source": [
    "## Create the Trainer Client\n",
    "\n",
    "Initialize the TrainerClient with authentication configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client with authentication\n",
    "api_client = k8s.ApiClient(configuration)\n",
    "\n",
    "backend_config = KubernetesBackendConfig(\n",
    "    client_configuration=api_client.configuration,\n",
    ")\n",
    "\n",
    "client = TrainerClient(backend_config)\n",
    "print(\"‚úÖ TrainerClient created\")\n",
    "\n",
    "# Get the torch-distributed runtime\n",
    "runtime = client.backend.get_runtime(\"torch-distributed\")\n",
    "print(f\"‚úÖ Using runtime: {runtime.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5d84c",
   "metadata": {},
   "source": [
    "## Submit the Training Job with TransformersTrainer\n",
    "\n",
    "Now we create and submit the distributed training job. The `TransformersTrainer` wraps your training function and handles all the distributed training setup.\n",
    "\n",
    "### Job Configuration\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `num_nodes` | 2 | Number of GPU nodes for distributed training |\n",
    "| `nvidia.com/gpu` | 1 | GPUs per node |\n",
    "| `cpu` | 4 | CPU cores per node |\n",
    "| `memory` | 16Gi | Memory per node |\n",
    "| `enable_progression_tracking` | True | Enable real-time progress monitoring (default) |\n",
    "| `metrics_poll_interval_seconds` | 30 | How often the dashboard polls for metrics |\n",
    "\n",
    "### PVC Mounting\n",
    "\n",
    "We use `PodTemplateOverrides` to mount the shared PVC in training pods:\n",
    "- **Mount Path:** `/opt/app-root/src` - Where training pods access model, data, and checkpoints\n",
    "- **PVC Name:** `shared` - The ReadWriteMany PVC containing our data\n",
    "\n",
    "This ensures all training nodes can access the same model weights, dataset, and can write checkpoints to a shared location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer.options.kubernetes import (\n",
    "    ContainerOverride,\n",
    "    PodSpecOverride,\n",
    "    PodTemplateOverride,\n",
    "    PodTemplateOverrides,\n",
    ")\n",
    "\n",
    "# Create TransformersTrainer with progress tracking (enabled by default)\n",
    "# Checkpointing is handled in the training function - saves to shared PVC\n",
    "trainer = TransformersTrainer(\n",
    "    func=train_func,\n",
    "    num_nodes=2,\n",
    "    resources_per_node={\n",
    "        \"nvidia.com/gpu\": 1,\n",
    "        \"cpu\": \"4\",\n",
    "        \"memory\": \"16Gi\",\n",
    "    },\n",
    "    # Progress tracking is enabled by default\n",
    "    enable_progression_tracking=True,\n",
    "    metrics_poll_interval_seconds=30,\n",
    ")\n",
    "\n",
    "# Submit the training job with PVC mount for model, data, and checkpoints\n",
    "job_name = client.train(\n",
    "    trainer=trainer,\n",
    "    runtime=runtime,\n",
    "    options=[\n",
    "        PodTemplateOverrides(\n",
    "            PodTemplateOverride(\n",
    "                target_jobs=[\"node\"],\n",
    "                spec=PodSpecOverride(\n",
    "                    volumes=[\n",
    "                        {\n",
    "                            \"name\": \"shared\",\n",
    "                            \"persistentVolumeClaim\": {\"claimName\": PVC_NAME},\n",
    "                        },\n",
    "                    ],\n",
    "                    containers=[\n",
    "                        ContainerOverride(\n",
    "                            name=\"node\",\n",
    "                            volume_mounts=[\n",
    "                                {\"name\": \"shared\", \"mountPath\": TRAINING_POD_PATH},\n",
    "                            ],\n",
    "                        )\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "print(f\"‚úÖ Training job submitted: {job_name}\")\n",
    "print(\"üìä Progress tracking: ENABLED (auto-injected by TransformersTrainer)\")\n",
    "print(f\"üíæ Checkpoints saved to: {TRAINING_CHECKPOINTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35992cdb",
   "metadata": {},
   "source": [
    "## Follow Job Logs\n",
    "\n",
    "Let's fetch our job logs to make sure training is going as expected. The logs will stream in real-time as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e665bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream logs (press Ctrl+C to stop if you want to continue with other cells)\n",
    "for logline in client.get_job_logs(job_name, follow=True):\n",
    "    print(logline, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc2edb",
   "metadata": {},
   "source": [
    "## Get Job Status\n",
    "\n",
    "Check the final status of the training job after completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615411cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check job status\n",
    "job = client.get_job(job_name)\n",
    "print(\"Final TrainJob Status:\")\n",
    "print(f\"   Name: {job.name}\")\n",
    "print(f\"   Status: {job.status}\")\n",
    "print(f\"   Created: {job.creation_timestamp}\")\n",
    "print(f\"   Nodes: {job.num_nodes}\")\n",
    "print(f\"   Runtime: {job.runtime.name}\")\n",
    "\n",
    "if job.steps:\n",
    "    print(\"   Steps:\")\n",
    "    for step in job.steps:\n",
    "        print(f\"     - {step.name}: {step.status}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132f69b",
   "metadata": {},
   "source": [
    "## Check Training Progress\n",
    "\n",
    "View detailed progress metrics from the TrainJob annotations. The Kubeflow controller polls the training pods every 30 seconds and writes metrics to these annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from kubernetes import client as k8s\n",
    "\n",
    "custom_api = k8s.CustomObjectsApi(api_client)\n",
    "\n",
    "# Get current namespace\n",
    "try:\n",
    "    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "        job_namespace = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    # Fallback: use oc project -q\n",
    "    import subprocess\n",
    "\n",
    "    result = subprocess.run(\"oc project -q\", shell=True, capture_output=True, text=True)\n",
    "    job_namespace = result.stdout.strip() if result.returncode == 0 else \"default\"\n",
    "\n",
    "print(f\"üìç Namespace: {job_namespace}\")\n",
    "\n",
    "try:\n",
    "    trainjob = custom_api.get_namespaced_custom_object(\n",
    "        group=\"trainer.kubeflow.org\",\n",
    "        version=\"v1alpha1\",\n",
    "        namespace=job_namespace,\n",
    "        plural=\"trainjobs\",\n",
    "        name=job_name,\n",
    "    )\n",
    "\n",
    "    annotations = trainjob.get(\"metadata\", {}).get(\"annotations\", {})\n",
    "\n",
    "    print(f\"\\nTrainJob Progress for {job_name}:\\n\")\n",
    "\n",
    "    # Progression tracking config\n",
    "    print(\"Config:\")\n",
    "    print(\n",
    "        f\"  tracking-enabled: {annotations.get('trainer.opendatahub.io/progression-tracking', 'N/A')}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  metrics-port: {annotations.get('trainer.opendatahub.io/metrics-port', 'N/A')}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  poll-interval: {annotations.get('trainer.opendatahub.io/metrics-poll-interval', 'N/A')}s\"\n",
    "    )\n",
    "\n",
    "    # Progression metrics from trainerStatus annotation\n",
    "    trainer_status = annotations.get(\"trainer.opendatahub.io/trainerStatus\")\n",
    "    if trainer_status:\n",
    "        progress = json.loads(trainer_status)\n",
    "        print(\"\\nMetrics:\")\n",
    "        print(f\"  progress: {progress.get('progressPercentage', 'N/A')}%\")\n",
    "        print(\n",
    "            f\"  step: {progress.get('currentStep', 'N/A')}/{progress.get('totalSteps', 'N/A')}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  epoch: {progress.get('currentEpoch', 'N/A')}/{progress.get('totalEpochs', 'N/A')}\"\n",
    "        )\n",
    "        print(f\"  remaining: {progress.get('estimatedRemainingSeconds', 'N/A')}s\")\n",
    "\n",
    "        train_metrics = progress.get(\"trainMetrics\", {})\n",
    "        if train_metrics:\n",
    "            print(f\"  loss: {train_metrics.get('loss', 'N/A')}\")\n",
    "            print(f\"  learning_rate: {train_metrics.get('learning_rate', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nAll annotations:\\n{json.dumps(annotations, indent=2)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8547474d",
   "metadata": {},
   "source": [
    "## Test the Trained Model\n",
    "\n",
    "After training completes, we load the fine-tuned model from the checkpoint saved on the shared PVC.\n",
    "\n",
    "### Checkpoint Structure\n",
    "\n",
    "The training function saves checkpoints with this structure:\n",
    "```\n",
    "/opt/app-root/src/shared/checkpoints/progress-tracking/\n",
    "‚îú‚îÄ‚îÄ checkpoint-20/     # Intermediate checkpoint at step 20\n",
    "‚îú‚îÄ‚îÄ checkpoint-32/     # Checkpoint at final step\n",
    "‚îî‚îÄ‚îÄ final/             # Final merged model ready for inference\n",
    "```\n",
    "\n",
    "### Testing the Model\n",
    "\n",
    "We'll load the fine-tuned model and test it with an instruction prompt using the same format as the training data:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "<your instruction here>\n",
    "\n",
    "### Response:\n",
    "<model generates response>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c43b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_recent_checkpoint(output_dir):\n",
    "    \"\"\"Find the most recently created checkpoint directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        raise FileNotFoundError(f\"Output directory not found: {output_dir}\")\n",
    "\n",
    "    checkpoint_dirs = [\n",
    "        os.path.join(output_dir, d)\n",
    "        for d in os.listdir(output_dir)\n",
    "        if os.path.isdir(os.path.join(output_dir, d))\n",
    "        and (d.startswith(\"checkpoint-\") or d == \"final\")\n",
    "    ]\n",
    "\n",
    "    if not checkpoint_dirs:\n",
    "        raise FileNotFoundError(f\"No checkpoints found in {output_dir}\")\n",
    "\n",
    "    # Prefer 'final' if it exists\n",
    "    final_path = os.path.join(output_dir, \"final\")\n",
    "    if os.path.exists(final_path):\n",
    "        return final_path\n",
    "\n",
    "    return max(checkpoint_dirs, key=os.path.getctime)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Checkpoint utility defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc49102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load the trained model\n",
    "final_checkpoint = find_most_recent_checkpoint(CHECKPOINTS_PATH)\n",
    "print(f\"üìÇ Loading checkpoint from: {final_checkpoint}\")\n",
    "\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    final_checkpoint, trust_remote_code=True\n",
    ")\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    final_checkpoint,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")\n",
    "print(f\"üìä Model parameters: {trained_model.num_parameters():,}\")\n",
    "\n",
    "# Test the model\n",
    "test_prompt = \"### Instruction:\\nExplain what machine learning is in one sentence.\\n\\n### Response:\"\n",
    "\n",
    "print(\"\\nüìù Testing model with prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\nü§ñ Model response:\")\n",
    "\n",
    "inputs = trained_tokenizer(test_prompt, return_tensors=\"pt\").to(trained_model.device)\n",
    "\n",
    "# Remove token_type_ids if present (not used by some models like Qwen)\n",
    "if \"token_type_ids\" in inputs:\n",
    "    del inputs[\"token_type_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "response = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response.replace(test_prompt, \"\").strip())\n",
    "\n",
    "print(\"\\n‚úÖ Model test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2127db",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the training job and free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eccd277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "client.delete_job(name=job_name)\n",
    "print(f\"‚úÖ Job {job_name} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b89f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "gc.collect()\n",
    "print(\"‚úÖ Resources freed, CUDA cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415804b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully completed a distributed fine-tuning job with real-time progress tracking on OpenShift AI.\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| ‚úÖ Model Download | Downloaded Qwen 2.5 1.5B Instruct to shared PVC |\n",
    "| ‚úÖ Dataset Preparation | Processed Stanford Alpaca dataset for instruction-tuning |\n",
    "| ‚úÖ Distributed Training | Ran 2-node distributed training with PyTorch DDP |\n",
    "| ‚úÖ Progress Tracking | Monitored real-time metrics via SDK and Dashboard |\n",
    "| ‚úÖ Checkpointing | Saved model checkpoints to shared PVC |\n",
    "| ‚úÖ Model Testing | Loaded and tested the fine-tuned model |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **TransformersTrainer** automatically instruments your HuggingFace `Trainer` with:\n",
    "   - `KubeflowProgressCallback` for real-time metrics (HTTP endpoint on port 28080)\n",
    "   - Progress visible in OpenShift AI Dashboard without any code changes\n",
    "\n",
    "2. **Shared PVC Strategy** for distributed training:\n",
    "   - Pre-download model and dataset to shared RWX PVC from workbench\n",
    "   - Training pods mount the same PVC and access data locally (offline mode)\n",
    "   - Checkpoints are written to shared storage for durability\n",
    "\n",
    "3. **Supported Trainers**:\n",
    "   - `transformers.Trainer` - Standard HuggingFace trainer\n",
    "   - `trl.SFTTrainer` - TRL's supervised fine-tuning trainer\n",
    "\n",
    "### TransformersTrainer Quick Reference\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `func` | Training function using `transformers.Trainer` | Required |\n",
    "| `num_nodes` | Number of distributed training nodes | Required |\n",
    "| `resources_per_node` | GPU, CPU, memory per node | Required |\n",
    "| `enable_progression_tracking` | Enable real-time metrics server | `True` |\n",
    "| `metrics_poll_interval_seconds` | How often controller polls metrics | `30` |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Scale Up:** Increase `num_nodes` for larger models or datasets\n",
    "- **Use LoRA:** Add PEFT/LoRA for memory-efficient fine-tuning\n",
    "- **Try Other Models:** This pattern works with any HuggingFace model\n",
    "- **Enable JIT Checkpointing:** Use `enable_jit_checkpoint=True` for automatic checkpoint saving on preemption\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Kubeflow Trainer Documentation](https://www.kubeflow.org/docs/components/trainer/)\n",
    "- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)\n",
    "- [Stanford Alpaca Dataset](https://huggingface.co/datasets/tatsu-lab/alpaca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
